#!/usr/bin/env python3

"""
Local Memory Manager

ç®¡ç†çŸ­æœŸå·¥ä½œè®°å¿†ï¼ˆLocal Memoryï¼‰ï¼Œç”¨äºåœ¨è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼š
- ç»´æŠ¤å…¨å±€çŠ¶æ€ï¼ˆå½“å‰ä»£æ•°ã€æœ€ä½³æ€§èƒ½ã€æœ€ä½³è§£IDã€å½“å‰æ–¹æ³•ï¼‰
- è®°å½•å°è¯•è¿‡çš„é«˜å±‚æ–¹å‘åŠå…¶æˆè´¥ï¼ˆdirection boardï¼‰
- æ²‰æ·€å¯è¿ç§»çš„æˆåŠŸ/å¤±è´¥ç»éªŒï¼ˆreasoning_bankï¼‰

è¯¥æ¨¡å—å‚è€ƒ reasoningbank çš„ Memory è®¾è®¡æ€æƒ³ï¼Œæä¾›ç»“æ„åŒ–çš„ JSON å­˜å‚¨ä¸å¢é‡æ›´æ–°ï¼Œ
å¹¶åœ¨éœ€è¦æ—¶è°ƒç”¨ LLM è¿›è¡Œè®°å¿†æç‚¼ï¼ˆExtractionï¼‰ã€‚
"""

from __future__ import annotations

import json
import math
from pathlib import Path
from typing import Any

from .llm_client import LLMClient
from .se_logger import get_se_logger


class LocalMemoryManager:
    """
    æœ¬åœ°è®°å¿†ç®¡ç†å™¨ï¼ˆJSON åç«¯ï¼‰

    å­˜å‚¨ç»“æ„ï¼ˆç¤ºä¾‹ï¼‰ï¼š
    {
    "global_status": {
        "current_generation": 5,
        "current_solution_id": "Gen5_Sol_2",
        "best_solution_id": "Gen3_Sol_4"
    },

    "direction_board": [
        {
        "direction": "Use faster input/output instead of standard C++ streams.",
        "description": "For input-heavy C++ problems, replace cin/cout with faster I/O patterns such as scanf/printf or enabling ios::sync_with_stdio(false) and cin.tie(nullptr). This reduces per-call overhead and improves constant factors when reading or writing large volumes of data.",
        "status": "Success",               // Success | Failed | Neutral | Untried
        "success_count": 2,
        "failure_count": 1,
        "evidence": [
            {
            "solution_id": "Gen5_Sol_2",
            "metrics_delta": "Runtime: 150ms -> 120ms (-20%).",
            "code_change": "Replaced cin/cout with scanf/printf for all integer reads.",
            "context": "C++ solution with N up to 2e5 where input reading dominated runtime.",
            "step_outcome": "Success"
            }
        ]
        }
    ],

    "experience_library": [
        {
        "type": "Success",                 // Success | Failure | Neutral
        "title": "Bitwise modulo for power-of-two MOD",
        "description": "When MOD is a power of two, using x & (MOD-1) is faster than x % MOD and is mathematically equivalent.",
        "content": "- Only apply when MOD = 2^k.\n- Replacing division-based modulo with bitwise AND removes expensive division operations in tight loops.\n- This can significantly improve performance in DP transitions or frequency counting loops.\n- Must avoid using this trick when MOD can change or is not guaranteed to be a power of two.",
        "evidence": [
            {
            "solution_id": "Gen5_Sol_2",
            "code_change": "Changed dp[i] % 1024 -> dp[i] & 1023 in the main DP loop.",
            "metrics_delta": "Runtime: 150ms -> 120ms (-20%).",
            "context": "Hot DP loop with fixed MOD=1024, N up to 1e5."
            }
        ]
        }
    ]
    }
    """

    def __init__(
        self,
        memory_path: str | Path,
        llm_client: LLMClient | None = None,
        token_limit: int = 3000,
        format_mode: str = "short",
    ) -> None:
        """
        åˆå§‹åŒ–æœ¬åœ°è®°å¿†ç®¡ç†å™¨ã€‚

        Args:
            memory_path: è®°å¿†åº“ JSON æ–‡ä»¶è·¯å¾„ã€‚
            llm_client: å¯é€‰çš„ LLM å®¢æˆ·ç«¯ï¼Œç”¨äºè¿›è¡Œè®°å¿†æç‚¼ã€‚
            token_limit: è§¦å‘å‹ç¼©çš„è¿‘ä¼¼ token/å­—ç¬¦é˜ˆå€¼ã€‚
        """
        self.path = Path(memory_path)
        self.llm_client = llm_client
        self.token_limit = int(token_limit)
        self.logger = get_se_logger("local_memory", emoji="ğŸ§ ")
        self.format_mode = str(format_mode or "short").lower()

    def _entry_include_keys(self) -> set[str] | None:
        try:
            if str(self.format_mode).lower() == "full":
                return None
        except Exception:
            pass
        return {"code", "perf_metrics"}

    def initialize(self) -> None:
        """ç¡®ä¿è®°å¿†åº“æ–‡ä»¶å­˜åœ¨ï¼Œè‹¥ä¸å­˜åœ¨åˆ™åˆ›å»ºç©ºç»“æ„ã€‚"""
        self.path.parent.mkdir(parents=True, exist_ok=True)
        if not self.path.exists():
            empty = {
                "global_status": {
                    "current_generation": 0,
                    "current_solution_id": None,
                    "best_solution_id": None,
                },
                "direction_board": [],
                "experience_library": [],
            }
            with open(self.path, "w", encoding="utf-8") as f:
                json.dump(empty, f, ensure_ascii=False, indent=2)
            self.logger.info(f"åˆå§‹åŒ–æœ¬åœ°è®°å¿†åº“: {self.path}")

    def load(self) -> dict[str, Any]:
        """åŠ è½½è®°å¿†åº“ JSONã€‚"""
        try:
            with open(self.path, encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError:
            return {"global_status": {}, "direction_board": [], "experience_library": []}
        except Exception as e:
            self.logger.warning(f"åŠ è½½æœ¬åœ°è®°å¿†åº“å¤±è´¥: {e}")
            return {"global_status": {}, "direction_board": [], "experience_library": []}

    def save(self, memory: dict[str, Any]) -> None:
        """ä¿å­˜è®°å¿†åº“ JSONã€‚"""
        try:
            with open(self.path, "w", encoding="utf-8") as f:
                json.dump(memory, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.error(f"ä¿å­˜æœ¬åœ°è®°å¿†åº“å¤±è´¥: {e}")
            raise

    def render_as_markdown(self, memory: dict[str, Any]) -> str:
        """
        å°†ç»“æ„åŒ–è®°å¿†æ¸²æŸ“ä¸ºç®€æ´çš„ Markdown æ–‡æœ¬ï¼Œä¾¿äºæ³¨å…¥ System Promptã€‚
        """
        dirs = memory.get("direction_board") or []
        bank = memory.get("experience_library") or []

        lines: list[str] = []

        # æ€»ä½“è¯´æ˜
        lines.append("## Local Memory (Evolution History)")
        lines.append("")
        lines.append("This is the accumulated knowledge from previous optimization attempts on THIS problem.")
        lines.append("**How to use this memory:**")
        lines.append("1. **Learn from successful patterns**: Apply or Improve insights from Success experiences.")
        lines.append("2. **Avoid repeated failures**: Do NOT retry directions that have failed multiple times.")
        lines.append(
            "3. **Explore new directions**: If existing directions are exhausted, try fundamentally different approaches."
        )
        lines.append("")

        # Tried Directions éƒ¨åˆ†
        lines.append("### Tried Directions (Strategy Board)")
        lines.append("")
        lines.append("These are high-level optimization strategies that have been attempted.")
        lines.append("- **[Success]**: This direction worked well. Consider building upon it.")
        lines.append("- **[Failed]**: This direction did NOT work. Do NOT retry the same approach.")
        lines.append("- **[Neutral]**: No effect; may be worth exploring with modifications.")
        lines.append("- **(âœ“N âœ—M)**: N successful attempts, M failed attempts.")
        lines.append("")

        if dirs:
            for d in dirs:
                status = d.get("status", "Unknown")
                succ = d.get("success_count", 0)
                fail = d.get("failure_count", 0)
                lines.append(f"- [{status}] {d.get('direction', '')} (âœ“{succ} âœ—{fail}) â€” {d.get('description', '')}")
        else:
            lines.append("- (No directions recorded yet)")
        lines.append("")

        # Learned Patterns éƒ¨åˆ†
        lines.append("### Learned Patterns (Experience Library)")
        lines.append("")
        lines.append("These are specific insights extracted from successful/failed attempts.")
        lines.append("- **âœ… Apply**: Proven techniques that improve performance. Use or Improve Them!")
        lines.append("- **âš ï¸ Avoid**: Anti-patterns that caused failures. Do NOT repeat these mistakes!")
        lines.append("")

        if bank:
            for item in bank:
                item_type = str(item.get("type", "")).strip()
                title = str(item.get("title", "")).strip()
                description = str(item.get("description", "")).strip()
                content = item.get("content", "")

                # æ ¹æ®ç±»å‹æ ¼å¼åŒ–æ ‡é¢˜å‰ç¼€ï¼Œä½¿æˆåŠŸ/å¤±è´¥æ›´æ¸…æ™°
                if item_type.lower() == "failure":
                    prefix = "âš ï¸ Avoid"
                    type_label = "Anti-pattern"
                elif item_type.lower() == "success":
                    prefix = "âœ… Apply"
                    type_label = "Better Practice"
                else:
                    prefix = "ğŸ“"
                    type_label = "Observation"

                lines.append(f"#### {prefix}: {title}")
                lines.append(f"- ({type_label}) {description}")
                lines.append(f"- Detail: {content}")
                lines.append("")
        else:
            lines.append("- (No patterns learned yet)")

        return "\n".join(lines)

    def _estimate_chars(self, memory: dict[str, Any]) -> int:
        """ç²—ç•¥ä¼°è®¡è®°å¿†ä½“é‡ï¼ˆæŒ‰å­—ç¬¦è®¡ï¼‰ã€‚"""
        try:
            return len(self.render_as_markdown(memory))
        except Exception:
            return 0

    def _format_metrics_delta(self, perf_old: float | None, perf_new: float | None) -> str:
        """å°†æ€§èƒ½å˜åŒ–æ ¼å¼åŒ–ä¸ºæ˜“è¯»å­—ç¬¦ä¸²ã€‚"""
        try:
            if perf_old is None or perf_new is None:
                return "N/A"
            if math.isinf(perf_old) and not math.isinf(perf_new):
                return f"Runtime: inf -> {perf_new}"
            if math.isinf(perf_new):
                return f"Runtime: {perf_old} -> inf"
            delta = perf_new - perf_old
            pct = (delta / perf_old * 100.0) if perf_old and not math.isinf(perf_old) else None
            if pct is None:
                return f"Runtime: {perf_old} -> {perf_new}"
            sign = "+" if pct >= 0 else ""
            return f"Runtime: {perf_old} -> {perf_new} ({sign}{pct:.1f}%)"
        except Exception:
            return "N/A"

    def _build_extraction_prompts(
        self,
        problem_description: str | None,
        perf_old: float | None,
        perf_new: float | None,
        source_entries: list[dict[str, Any]] | None,
        current_entry: dict[str, Any] | None,
        best_entry: dict[str, Any] | None,
        current_directions: list[dict[str, Any]],
        language: str = "",
        optimization_target: str = "",
        current_solution_id: str | None = None,
    ) -> tuple[str, str]:
        """
        æ„é€ è®°å¿†æç‚¼çš„ System/User æç¤ºè¯ã€‚
        æ ¹æ®æ€§èƒ½å˜åŒ–åˆ†æµè¿›å…¥ Success æˆ– Failure åˆ†æ”¯ã€‚

        å¯¹äºåˆå§‹è§£ï¼ˆæ—  perf_oldï¼‰ï¼š
        - å¦‚æœ perf_new ä¸ä¸º infï¼Œè§†ä¸º Successï¼ˆåŸºçº¿å»ºç«‹æˆåŠŸï¼‰
        - å¦‚æœ perf_new ä¸º infï¼Œè§†ä¸º Failureï¼ˆåŸºçº¿å»ºç«‹å¤±è´¥ï¼‰
        """
        # 1. Metric Analysis
        perf_diff = 0.0

        if perf_old is not None and perf_new is not None:
            # Handle inf
            if math.isinf(perf_old) and not math.isinf(perf_new):
                perf_diff = float("inf")  # Improvement
            elif not math.isinf(perf_old) and math.isinf(perf_new):
                perf_diff = float("-inf")  # Regression
            elif math.isinf(perf_old) and math.isinf(perf_new):
                perf_diff = 0.0
            else:
                perf_diff = perf_old - perf_new
        elif perf_new is not None:
            # åˆå§‹è§£ï¼šæ ¹æ® perf_new æ˜¯å¦ä¸º inf åˆ¤æ–­æˆåŠŸ/å¤±è´¥
            if not math.isinf(perf_new):
                # åˆå§‹è§£æˆåŠŸï¼ˆæœ‰æœ‰æ•ˆæ€§èƒ½æ•°æ®ï¼‰ï¼Œè§†ä¸ºæ­£å‘
                perf_diff = float("inf")  # ä½œä¸º Success å¤„ç†
            else:
                # åˆå§‹è§£å¤±è´¥ï¼ˆæ€§èƒ½ä¸º infï¼‰ï¼Œè§†ä¸ºè´Ÿå‘
                perf_diff = float("-inf")  # ä½œä¸º Failure å¤„ç†

        # 2. Extraction Branch - ç»Ÿä¸€ä½¿ç”¨ Success/Failure åˆ†æ”¯
        # ä¸å†å•ç‹¬è°ƒç”¨ _build_initial_promptï¼Œåˆå§‹è§£æ ¹æ® perf_diff å½’å…¥ç›¸åº”åˆ†æ”¯
        if perf_diff > 0:
            return self._build_success_prompt(
                problem_description,
                perf_old,
                perf_new,
                perf_diff,
                source_entries,
                current_entry,
                best_entry,
                current_directions,
                language,
                optimization_target,
                current_solution_id,
            )
        else:
            return self._build_failure_prompt(
                problem_description,
                perf_old,
                perf_new,
                perf_diff,
                source_entries,
                current_entry,
                best_entry,
                current_directions,
                language,
                optimization_target,
                current_solution_id,
            )

    def _build_success_prompt(
        self,
        problem,
        perf_old,
        perf_new,
        perf_diff,
        source_entries,
        current_entry,
        best_entry,
        directions,
        language,
        target,
        current_solution_id,
    ) -> tuple[str, str]:
        # 1. System Prompt
        system_prompt = """You are an expert Algorithm Optimization Specialist. You have just observed an evolutionary step where an agent **attempted to optimize** a code solution and the **metrics show an improvement** (or at least not a clear regression).

Your job is NOT to log every tiny change. Your job is to maintain:
- a **high-level strategy board** (`direction_board`), and
- an **experience library** (`experience_library`)
that together guide future evolution.

---

## Goal

Given the previous and current solutions, you must:

1. Decide whether this step is truly a **Success**, or actually **Neutral** (e.g., noise, trivial refactor).
2. If (and only if) there are **strategy-level changes**, extract up to 3 new:
   - **Direction items**: reusable optimization strategies that can be tried again on other solutions.
   - **Memory items**: distilled reasoning patterns that explain *why* certain strategies work.

This memory is local to a single problem and will be shown to the model in later steps to encourage **diverse strategy exploration**, not to duplicate the same ideas.

---

## Definitions

- **Strategy-level change**:
  - Switching algorithms (e.g., brute force â†’ two-pointer, BFS â†’ Dijkstra, naive DP â†’ optimized DP).
  - Changing core data structures (e.g., vector â†’ bitset, list â†’ array, unordered_map â†’ array-based counter).
  - Applying a clear performance trick (e.g., fast I/O, precomputation, caching, reducing passes over the array).
  - Changing memory layout or loop structure in a way that affects asymptotics or constant factors in a hotspot.

- **Non-strategy changes (DO NOT create directions for these)**:
  - Renaming variables, reformatting, reordering independent statements.
  - Small cosmetic refactors that do not change complexity or memory access patterns.
  - Pure measurement noise: identical code with slightly different runtimes.

---

## Very Important Rules

1. **You may return ZERO new directions and ZERO new memories.**
   - This is the correct behavior when no strategy-level change happened.

2. **Do NOT create directions about measurement noise or â€œno changeâ€.**
   - The following are explicitly forbidden as directions:
     - "No Change, OS Jitter"
     - "Measurement noise"
     - "Same code as previous solution"

3. **Noise vs Success vs Neutral**:
   - If the improvement is within typical measurement jitter, and there is *no* meaningful strategy change, treat the step as **Neutral**.
   - Only mark `"step_outcome": "Success"` when:
     - There is a real metric improvement **and**
     - You can tie it to a strategy-level code change.

4. **Rich, semantic content**:
   - `direction` should look like a clear strategy name that could appear on a "strategy board".
   - `description` should be 1â€“3 sentences explaining:
     - what the strategy does,
     - when to use it,
     - and potential trade-offs or risks.
   - For **Success** memory items:
     - `title`: Describe the successful technique (e.g., "Use rolling array DP for space optimization")
     - `content`: Explain **WHY** it works and **WHAT insight** makes it effective. Focus on the key reasoning.

5. **Cardinality constraints**:
   - At most 3 `new_direction_items`.
   - At most 3 `new_memory_items`.
   - Arrays can be empty (`[]`).

---

## Input Data Provided

You will be given:

1. **Optimization Target**: e.g., runtime, memory, integral.
2. **Language**: e.g., C++, Python.
3. **Problem Description**: The algorithmic problem being solved.
4. **Source Solutions**: Parent code(s), summaries and metrics before mutation.
5. **Current Solution**: Mutated code, summary and metrics after mutation.
6. **Best Solution**: The global best solution so far (for context).
7. **Current Directions**: The current snapshot of the strategy board for this problem.

Use the diffs between Source and Current solution to reason about what changed.

---

## Output Format

You must output a single JSON object **strictly** adhering to this schema:

```json
{
  "thought_process": "Briefly explain your reasoning here.",

  "new_direction_items": [
    {
      "direction": "High-level strategy name.",
      "description": "1â€“3 sentences describing what was changed, why it is a reusable strategy, and when it applies.",
      "status": "Success | Neutral",
    }
  ],

  "new_memory_items": [
    {
      "type": "Success | Neutral",
      "title": "Concise title of the reasoning pattern.",
      "description": "One-sentence summary of the insight.",
      "content": "2â€“6 sentences explaining when to apply this, why it works, and any risks.",
    }
  ]
}
```

Notes:
- If there is no meaningful strategy-level change, set "step_outcome": "Neutral" and both arrays to [].
- Do not invent fake strategies just to fill the JSON.
        """
        # åˆ¤æ–­æ˜¯å¦æ˜¯åˆå§‹åŒ–åœºæ™¯ï¼ˆæ—  source entriesï¼‰
        is_initial = not source_entries

        if is_initial:
            # åˆå§‹åŒ–åœºæ™¯ï¼šè¯†åˆ«åŸºçº¿ç­–ç•¥
            user_template = """
## Mode: BASELINE INITIALIZATION

This is the **initial solution** (baseline). There is no previous version to compare against.
Your task is to **identify the core algorithmic strategy** used in the Current Solution and record it as the baseline.

## Guidelines for Baseline Extraction

1. **Identify Strategy**: Analyze the code. What is the core algorithmic paradigm? (e.g., Dynamic Programming, Greedy, BFS, Binary Search, Simulation, or naive Brute Force).
2. **Establish Baseline**: Create a direction item describing this fundamental approach with status "Baseline" or "Success".
3. **No Comparison Needed**: Since there's no source to compare, focus on identifying WHAT strategy the code uses, not HOW it changed.

## Optimization Target

{optimization_target}

## Language

{language}

## Problem Description

{problem_description}

## Current Solution (Baseline)

{current_solution}

## Best Solution

{best_solution}

## Current Directions (Strategy Board Snapshot)

{directions}
        """
        else:
            # å˜å¼‚åœºæ™¯ï¼šæ¯”è¾ƒ source å’Œ current
            user_template = """
## Optimization Target

{optimization_target}

The optimization target is **integral**:  
- Interpret this as the **integral of memory usage over runtime** for all test cases, i.e., the **area under the memoryâ€“time curve**.
- Your performance judgments should consider **both** runtime and memory, focusing on how each slot affects this **memoryâ€“time integral**, not just speed or memory in isolation.
- A slot that is slightly slower but uses much less memory can be better if it reduces the overall integral, and vice versa.

## Language

{language}

## Problem Description

{problem_description}

## Source Solutions

{source_solutions}

## Current Solution

{current_solution}

## Best Solution

{best_solution}

## Current Directions (Strategy Board Snapshot)

{directions}
        """
        # Build formatted texts using TrajPoolManager.format_entry
        try:
            from .traj_pool_manager import TrajPoolManager
        except Exception:
            TrajPoolManager = None  # type: ignore

        def _fmt_entry_text(entry: dict | None) -> str:
            try:
                if TrajPoolManager and isinstance(entry, dict):
                    lbl = str(entry.get("label") or entry.get("solution_id") or "current")
                    return TrajPoolManager.format_entry({lbl: entry}, include_keys=self._entry_include_keys())
            except Exception:
                pass
            return "N/A"

        def _fmt_entries_text(entries: list[dict] | None) -> str:
            if not entries:
                return "N/A"
            texts: list[str] = []
            for e in entries:
                t = _fmt_entry_text(e)
                if t and t != "N/A":
                    texts.append(t)
            return "\n\n".join(texts) if texts else "N/A"

        source_solutions_text = _fmt_entries_text(source_entries)
        current_solution_text = _fmt_entry_text(current_entry)
        best_solution_text = _fmt_entry_text(best_entry)

        # æ ¹æ®æ˜¯å¦æ˜¯åˆå§‹åŒ–åœºæ™¯é€‰æ‹©æ ¼å¼åŒ–å‚æ•°
        format_kwargs = {
            "optimization_target": str(target or "Runtime"),
            "language": str(language or "Unknown"),
            "problem_description": str(problem or "N/A"),
            "current_solution": current_solution_text,
            "best_solution": best_solution_text,
            "directions": json.dumps(directions or [], ensure_ascii=False),
        }
        if not is_initial:
            format_kwargs["source_solutions"] = source_solutions_text

        user_prompt = user_template.format(**format_kwargs)

        return system_prompt, user_prompt

    def _build_failure_prompt(
        self,
        problem,
        perf_old,
        perf_new,
        perf_diff,
        source_entries,
        current_entry,
        best_entry,
        directions,
        language,
        target,
        current_solution_id,
    ) -> tuple[str, str]:
        # 1. System Prompt
        system_prompt = """You are an expert Algorithm Optimization Specialist. You have just observed an evolutionary step where an agent **attempted to optimize** a code solution and the **metrics show a regression or incorrectness**.

Your job is NOT to log every tiny change. Your job is to maintain:
- a **high-level strategy board** (`direction_board`), and
- an **experience library** (`experience_library`)
that warn future steps about bad ideas.

---

## Goal

Given the previous and current solutions, you must:

1. Decide whether this step is truly a **Failure**, or actually **Neutral** (e.g., noise, trivial refactor).
2. If (and only if) there are **strategy-level changes that caused the regression**, extract up to 3 new:
   - **Direction items**: strategies that should be marked as Failed or risky in the current context.
   - **Memory items**: warnings or anti-patterns explaining *why* this approach failed and when to avoid it.

---

## Definitions

- **Strategy-level change**:
  - Same as in the Success case: algorithm switch, data structure switch, clear performance trick, major loop or memory layout change.
- **Non-strategy changes (DO NOT create directions for these)**:
  - Formatting, renaming, minor refactors with no impact on complexity or memory access.
  - Pure measurement noise with identical code.

---

## Very Important Rules

1. **You may return ZERO new directions and ZERO new memories.**
   - This is the correct behavior when no strategy-level change caused the regression.

2. **Do NOT create directions about measurement noise or â€œno changeâ€.**
   - Explicitly forbidden directions:
     - "No Change, OS Jitter"
     - "Measurement noise"
     - "Same code as previous solution"

3. **Noise vs Failure vs Neutral**:
   - If the regression is typical measurement jitter, and there is *no* meaningful strategy change, treat the step as **Neutral**.
   - Only mark `"step_outcome": "Failure"` when:
     - Runtime, memory, or correctness clearly got worse **and**
     - You can tie it to a strategy-level change (e.g., added redundant checks, switched to a slower algorithm, broke edge cases).

4. **Rich, semantic content**:
   - Directions should describe *what strategy went wrong* (e.g., "aggressive pruning without correctness proof", "using recursion with unbounded depth").
   - For **Failure** memory items:
     - `title`: Describe the **SPECIFIC mistake**, NOT just the approach name.
       - BAD: "BFS implementation" (too vague)
       - GOOD: "BFS without boundary check causes index out of bounds"
       - GOOD: "Recursive factorial without memoization causes TLE for large N"
     - `content`: Explain **WHY** it failed and **WHAT specific condition** triggered the failure.

5. **Cardinality constraints**:
   - At most 3 `new_direction_items`.
   - At most 3 `new_memory_items`.
   - Arrays can be empty (`[]`).

---

## Input Data Provided

Same as in the Success case:

1. **Optimization Target**
2. **Language**
3. **Problem Description**
4. **Source Solutions**
5. **Current Solution**
6. **Best Solution**
7. **Current Directions**

---

## Output Format

You must output a single JSON object **strictly** adhering to this schema:

```json
{
  "thought_process": "Briefly explain your reasoning here (max 2 sentences).",
  "step_outcome": "Failure | Neutral",

  "new_direction_items": [
    {
      "direction": "High-level description of the failed strategy.",
      "description": "1â€“3 sentences explaining what the strategy tried to do and why it is problematic in this context.",
      "status": "Failed | Neutral",

  "new_memory_items": [
    {
      "type": "Failure | Neutral",
      "title": "Start with 'Avoid ...' for Failure type (e.g., 'Avoid recursive solution without memoization').",
      "description": "One-sentence summary of why this approach is dangerous and should be avoided.",
      "content": "2â€“6 sentences explaining what went wrong, under what conditions it fails, and how to avoid it.",

}
```

Notes:
- If there is no meaningful strategy-level change, set "step_outcome": "Neutral" and both arrays to [].
- Do not mark previously successful strategies as failed just because one noisy run was slower.
        """
        # åˆ¤æ–­æ˜¯å¦æ˜¯åˆå§‹åŒ–åœºæ™¯ï¼ˆæ—  source entriesï¼‰
        is_initial = not source_entries

        if is_initial:
            # åˆå§‹åŒ–å¤±è´¥åœºæ™¯ï¼šåˆå§‹è§£å°±å¤±è´¥äº†ï¼ˆTLE/OOM/WAï¼‰
            user_template = """
## Mode: BASELINE INITIALIZATION FAILED

This is the **initial solution** (baseline), but it **failed** (TLE, OOM, WA, or other errors).
There is no previous version to compare against.

Your task is to:
1. **Identify what strategy the code attempted** (e.g., naive brute force, unoptimized DP, etc.)
2. **Record why it failed** as a warning for future iterations

## Guidelines for Failed Baseline

- Create a direction item with status "Failed" describing the attempted approach
- Create a memory item explaining why this approach doesn't work for this problem
- Focus on identifying the **root cause** of failure (time complexity too high? memory usage too large? edge case bug?)

## Optimization Target

{optimization_target}

## Language

{language}

## Problem Description

{problem_description}

## Current Solution (Failed Baseline)

{current_solution}

## Best Solution

{best_solution}

## Current Directions (Strategy Board Snapshot)

{directions}
        """
        else:
            # å˜å¼‚å¤±è´¥åœºæ™¯ï¼šæ¯”è¾ƒ source å’Œ current
            user_template = """    
## Optimization Target

{optimization_target}

The optimization target is **integral**:  
- Interpret this as the **integral of memory usage over runtime** for all test cases, i.e., the **area under the memoryâ€“time curve**.
- Your performance judgments should consider **both** runtime and memory, focusing on how each slot affects this **memoryâ€“time integral**, not just speed or memory in isolation.
- A slot that is slightly slower but uses much less memory can be better if it reduces the overall integral, and vice versa.


## Language

{language}

## Problem Description

{problem_description}

## Source Solutions

{source_solutions}

## Current Solution

{current_solution}

## Best Solution

{best_solution}

## Current Directions (Strategy Board Snapshot)

{directions}
        """
        # Build formatted texts using TrajPoolManager.format_entry
        try:
            from .traj_pool_manager import TrajPoolManager
        except Exception:
            TrajPoolManager = None  # type: ignore

        def _fmt_entry_text(entry: dict | None) -> str:
            try:
                if TrajPoolManager and isinstance(entry, dict):
                    lbl = str(entry.get("label") or entry.get("solution_id") or "current")
                    return TrajPoolManager.format_entry({lbl: entry}, include_keys=self._entry_include_keys())
            except Exception:
                pass
            return "N/A"

        def _fmt_entries_text(entries: list[dict] | None) -> str:
            if not entries:
                return "N/A"
            texts: list[str] = []
            for e in entries:
                t = _fmt_entry_text(e)
                if t and t != "N/A":
                    texts.append(t)
            return "\n\n".join(texts) if texts else "N/A"

        source_solutions_text = _fmt_entries_text(source_entries)
        current_solution_text = _fmt_entry_text(current_entry)
        best_solution_text = _fmt_entry_text(best_entry)

        # æ ¹æ®æ˜¯å¦æ˜¯åˆå§‹åŒ–åœºæ™¯é€‰æ‹©æ ¼å¼åŒ–å‚æ•°
        format_kwargs = {
            "optimization_target": str(target or "Runtime"),
            "language": str(language or "Unknown"),
            "problem_description": str(problem or "N/A"),
            "current_solution": current_solution_text,
            "best_solution": best_solution_text,
            "directions": json.dumps(directions or [], ensure_ascii=False),
        }
        if not is_initial:
            format_kwargs["source_solutions"] = source_solutions_text

        user_prompt = user_template.format(**format_kwargs)

        return system_prompt, user_prompt

    def _parse_llm_json(self, text: str) -> dict[str, Any]:
        """æå–å¹¶è§£æ LLM è¿”å›çš„ JSON å†…å®¹ã€‚"""
        content = (text or "").strip()
        if not content:
            msg = "ç©ºå“åº”å†…å®¹ï¼Œæ— æ³•è§£æä¸ºJSON"
            raise ValueError(msg)

        # å°è¯•ç›´æ¥è§£æå®Œæ•´JSON
        if content.startswith("{"):
            return json.loads(content)

        # å°è¯•æå–JSONç‰‡æ®µè¿›è¡Œè§£æ
        start_idx = content.find("{")
        end_idx = content.rfind("}") + 1
        if start_idx >= 0 and end_idx > start_idx:
            json_content = content[start_idx:end_idx]
            return json.loads(json_content)

        # æœªæ‰¾åˆ°å¯è§£æçš„JSONç‰‡æ®µ
        msg = "å“åº”ä¸­æœªæ‰¾åˆ°å¯è§£æçš„JSONå†…å®¹"
        raise ValueError(msg)

    def _validate_memory_response(self, data: dict[str, Any]) -> None:
        if not isinstance(data, dict):
            msg = "å“åº”æ•°æ®å¿…é¡»ä¸ºJSONå¯¹è±¡"
            raise ValueError(msg)
        # ä»…æ”¯æŒæ•°ç»„å½¢å¼è¿”å›
        if "new_direction_items" not in data:
            msg = "å“åº”æ ¼å¼ç¼ºå°‘é”®: new_direction_items"
            raise ValueError(msg)
        nd = data.get("new_direction_items")
        if nd is not None and not isinstance(nd, list):
            msg = "new_direction_itemså¿…é¡»ä¸ºåˆ—è¡¨"
            raise ValueError(msg)
        if isinstance(nd, list):
            for it in nd:
                if not isinstance(it, dict):
                    msg = "new_direction_itemsçš„å…ƒç´ å¿…é¡»ä¸ºå¯¹è±¡"
                    raise ValueError(msg)

        if "new_memory_items" in data:
            nm = data.get("new_memory_items")
            if nm is not None and not isinstance(nm, list):
                msg = "new_memory_itemså¿…é¡»ä¸ºåˆ—è¡¨"
                raise ValueError(msg)
            if isinstance(nm, list):
                for it in nm:
                    if not isinstance(it, dict):
                        msg = "new_memory_itemsçš„å…ƒç´ å¿…é¡»ä¸ºå¯¹è±¡"
                        raise ValueError(msg)

    def _normalize_extraction_response(self, resp: dict[str, Any]) -> tuple[list[dict[str, Any]], list[dict[str, Any]]]:
        """å°†LLMå“åº”ç»Ÿä¸€è½¬æ¢ä¸ºåˆ—è¡¨å½¢å¼ã€‚"""
        dirs: list[dict[str, Any]] = []
        mems: list[dict[str, Any]] = []
        try:
            single_dir = resp.get("new_direction_item")
            if isinstance(single_dir, dict):
                dirs.append(single_dir)
            multi_dir = resp.get("new_direction_items")
            if isinstance(multi_dir, list):
                dirs.extend([d for d in multi_dir if isinstance(d, dict)])
        except Exception:
            pass
        try:
            single_mem = resp.get("new_memory_item")
            if isinstance(single_mem, dict):
                mems.append(single_mem)
            multi_mem = resp.get("new_memory_items")
            if isinstance(multi_mem, list):
                mems.extend([m for m in multi_mem if isinstance(m, dict)])
        except Exception:
            pass
        return dirs, mems

    def _merge_direction_board(self, memory: dict[str, Any], new_items: list[dict[str, Any]]) -> None:
        """å°†æç‚¼çš„æ–¹å‘é¡¹ç›´æ¥æ’å…¥ direction_boardã€‚"""
        board: list[dict[str, Any]] = memory.get("direction_board") or []
        for raw in new_items:
            if not isinstance(raw, dict):
                continue
            direction = str(raw.get("direction") or "").strip()
            if not direction:
                continue
            description = str(raw.get("description") or "").strip()
            status = str(raw.get("status") or "Neutral").strip()
            evidence_src = raw.get("evidence") if isinstance(raw.get("evidence"), list) else []
            evidence = [e for e in evidence_src if isinstance(e, dict)]

            # æ ¹æ® status åˆå§‹åŒ–è®¡æ•°ï¼ˆå¦‚æœ LLM æ²¡æœ‰è¿”å›è®¡æ•°ï¼‰
            raw_success = raw.get("success_count")
            raw_failure = raw.get("failure_count")
            if raw_success is not None:
                success_count = int(raw_success)
            elif status.lower() in ("success", "baseline"):
                success_count = 1
            else:
                success_count = 0

            if raw_failure is not None:
                failure_count = int(raw_failure)
            elif status.lower() == "failed":
                failure_count = 1
            else:
                failure_count = 0

            board.append(
                {
                    "direction": direction,
                    "description": description,
                    "status": status,
                    "success_count": success_count,
                    "failure_count": failure_count,
                    "evidence": evidence,
                }
            )
        memory["direction_board"] = board

    def _merge_experience_library(self, memory: dict[str, Any], new_items: list[dict[str, Any]]) -> None:
        """å°†æç‚¼çš„ç»éªŒé¡¹ç›´æ¥æ’å…¥ experience_libraryã€‚"""
        library: list[dict[str, Any]] = memory.get("experience_library") or []
        for raw in new_items:
            if not isinstance(raw, dict):
                continue
            title = str(raw.get("title") or "").strip()
            if not title:
                continue
            typ = str(raw.get("type") or "Neutral").strip()
            description = str(raw.get("description") or "").strip()
            content = raw.get("content")
            evidence_src = raw.get("evidence") if isinstance(raw.get("evidence"), list) else []
            evidence = [e for e in evidence_src if isinstance(e, dict)]

            library.append(
                {
                    "type": typ,
                    "title": title,
                    "description": description,
                    "content": content,
                    "evidence": evidence,
                }
            )
        memory["experience_library"] = library

    def compress_if_needed(self, memory: dict[str, Any]) -> None:
        """
        å¦‚æœè®°å¿†ä½“é‡è¶…è¿‡é˜ˆå€¼ï¼Œåˆ†åˆ«å‹ç¼© direction_board å’Œ experience_libraryã€‚
        """
        try:
            if self._estimate_chars(memory) <= self.token_limit:
                return
            if not self.llm_client:
                self.logger.warning("LLMä¸å¯ç”¨ï¼Œè·³è¿‡è®°å¿†å‹ç¼©")
                return

            # åˆ†åˆ«å‹ç¼© direction_board å’Œ experience_library
            self._compress_direction_board(memory)
            self._compress_experience_library(memory)

            self.logger.info("LLMè®°å¿†å‹ç¼©å®Œæˆ")
        except Exception as e:
            self.logger.warning(f"å‹ç¼©è®°å¿†å¤±è´¥: {e}")

    def _compress_direction_board(self, memory: dict[str, Any]) -> None:
        """å‹ç¼© direction_boardã€‚"""
        direction_board = memory.get("direction_board") or []
        if len(direction_board) <= 3:
            return  # å¤ªå°‘ï¼Œä¸éœ€è¦å‹ç¼©

        sys_prompt, user_prompt = self._build_compress_direction_board_prompts(direction_board)
        last_error: str | None = None

        for attempt in range(1, 4):
            try:
                llm_response = self.llm_client.call_with_system_prompt(
                    system_prompt=sys_prompt,
                    user_prompt=user_prompt,
                    temperature=0.7,
                    max_tokens=None,  # ä½¿ç”¨é…ç½®ä¸­çš„ max_output_tokens
                    usage_context="memory.compress_directions",
                )
                self.logger.debug(f"LLMåŸå§‹å“åº” (å‹ç¼©directionsï¼Œç¬¬{attempt}æ¬¡):\n{llm_response}")
                llm_response = self.llm_client.clean_think_tags(llm_response)
                parsed = self._parse_llm_json(llm_response)

                db = parsed.get("direction_board")
                if isinstance(db, list):
                    memory["direction_board"] = db
                    self.logger.info(f"direction_board å‹ç¼©æˆåŠŸ: {len(direction_board)} -> {len(db)} æ¡")
                    return
            except ValueError as e:
                last_error = "invalid_response_format"
                self.logger.warning(f"direction_board å‹ç¼©è§£æå¤±è´¥ (ç¬¬{attempt}æ¬¡): {e}")
            except Exception as e:
                last_error = "llm_call_failed"
                self.logger.warning(f"direction_board å‹ç¼©è°ƒç”¨å¤±è´¥ (ç¬¬{attempt}æ¬¡): {e}")

        if last_error:
            self.logger.error(f"direction_board å‹ç¼©æœ€ç»ˆå¤±è´¥: {last_error}")

    def _compress_experience_library(self, memory: dict[str, Any]) -> None:
        """å‹ç¼© experience_libraryã€‚"""
        experience_library = memory.get("experience_library") or []
        if len(experience_library) <= 3:
            return  # å¤ªå°‘ï¼Œä¸éœ€è¦å‹ç¼©

        sys_prompt, user_prompt = self._build_compress_experience_library_prompts(experience_library)
        last_error: str | None = None

        for attempt in range(1, 4):
            try:
                llm_response = self.llm_client.call_with_system_prompt(
                    system_prompt=sys_prompt,
                    user_prompt=user_prompt,
                    temperature=0.7,
                    max_tokens=None,  # ä½¿ç”¨é…ç½®ä¸­çš„ max_output_tokens
                    usage_context="memory.compress_experiences",
                )
                self.logger.debug(f"LLMåŸå§‹å“åº” (å‹ç¼©experiencesï¼Œç¬¬{attempt}æ¬¡):\n{llm_response}")
                llm_response = self.llm_client.clean_think_tags(llm_response)
                parsed = self._parse_llm_json(llm_response)

                el = parsed.get("experience_library")
                if isinstance(el, list):
                    memory["experience_library"] = el
                    self.logger.info(f"experience_library å‹ç¼©æˆåŠŸ: {len(experience_library)} -> {len(el)} æ¡")
                    return
            except ValueError as e:
                last_error = "invalid_response_format"
                self.logger.warning(f"experience_library å‹ç¼©è§£æå¤±è´¥ (ç¬¬{attempt}æ¬¡): {e}")
            except Exception as e:
                last_error = "llm_call_failed"
                self.logger.warning(f"experience_library å‹ç¼©è°ƒç”¨å¤±è´¥ (ç¬¬{attempt}æ¬¡): {e}")

        if last_error:
            self.logger.error(f"experience_library å‹ç¼©æœ€ç»ˆå¤±è´¥: {last_error}")

    def _build_compress_direction_board_prompts(self, direction_board: list[dict[str, Any]]) -> tuple[str, str]:
        """æ„å»ºå‹ç¼© direction_board çš„ promptã€‚"""
        system_prompt = """You are compressing the **direction_board** (Strategy Board) of an evolutionary coding agent.

## Task
Consolidate and compress the list of tried strategies while preserving useful information.

## Rules

1. **Merge semantically similar strategies**
   - If multiple entries describe the same idea (e.g., "Use fast I/O", "Replace cin/cout with scanf"), merge them.
   - Rewrite as a clear, unique strategy name.

2. **IMPORTANT: Do NOT merge strategies with DIFFERENT failure modes**
   - "Precompute factorials caused OOM" and "Iterative computation caused TLE" are DIFFERENT, keep them separate.
   - "Edge case N=0 failed" and "Large N caused overflow" are DIFFERENT, keep them separate.

3. **Aggregate counts when merging**
   - When merging similar strategies, SUM their success_count and failure_count.
   - Update status based on aggregated counts:
     - "Success" if success_count > failure_count.
     - "Failed" if failure_count > success_count.
     - "Neutral" if counts are equal or evidence is weak.

4. **Prune low-value directions**
   - Remove vague entries (e.g., "optimize code a bit").
   - Remove noise entries (e.g., "OS jitter", "no code change").
   - Keep roughly **5â€“10** useful directions.

## Output Format

```json
{
  "thought_process": "Brief explanation.",
  "direction_board": [
    {
      "direction": "Strategy name",
      "description": "1â€“3 sentences explaining the strategy.",
      "status": "Success | Failed | Neutral",
      "success_count": int,
      "failure_count": int
    }
  ]
}
```
"""
        user_prompt = f"""## Current Direction Board

{json.dumps(direction_board, indent=2, ensure_ascii=False)}

## Task
Compress and consolidate the direction_board above. Output ONLY the valid JSON object.
"""
        return system_prompt, user_prompt

    def _build_compress_experience_library_prompts(self, experience_library: list[dict[str, Any]]) -> tuple[str, str]:
        """æ„å»ºå‹ç¼© experience_library çš„ promptã€‚"""
        system_prompt = """You are compressing the **experience_library** of an evolutionary coding agent.

## Task
Consolidate and compress the list of learned experiences while preserving actionable insights.

## Rules

1. **Merge overlapping experiences**
   - If multiple entries describe the same lesson, merge them into one stronger experience.

2. **IMPORTANT: Do NOT merge experiences with DIFFERENT root causes**
   - "Avoid recursion without memoization (TLE)" and "Avoid large array allocation (OOM)" are DIFFERENT lessons, keep them separate.
   - "Use iterative DP" and "Use rolling array to save memory" are DIFFERENT techniques, keep them separate.

3. **Content Guidelines by Type**

   **For Success type:**
   - Title: Describe the successful technique/approach (e.g., "Use rolling array DP for space optimization")
   - Content: Explain WHY it works and WHAT insight makes it effective
   - Focus on: What was the key insight? Under what conditions does this work?

   **For Failure type:**
   - Title: Describe the SPECIFIC mistake, not just the approach (e.g., "BFS without boundary check causes index out of bounds", NOT just "BFS implementation")
   - Content: Explain WHY it failed and WHAT specific condition triggered the failure
   - Focus on: What exactly went wrong? What should be checked/avoided?

4. **Filter out trivial items**
   - Remove entries that only reflect measurement noise.
   - Remove entries with negligible effect and no actionable lesson.
   - Keep roughly **5â€“8** useful experiences.

## Output Format

```json
{
  "thought_process": "Brief explanation (1-2 sentences).",
  "experience_library": [
    {
      "type": "Success | Failure | Neutral",
      "title": "Specific, descriptive title",
      "description": "One-sentence summary of the insight/lesson.",
      "content": "2â€“6 sentences explaining when/why this works or fails."
    }
  ]
}
```
"""
        user_prompt = f"""## Current Experience Library

{json.dumps(experience_library, indent=2, ensure_ascii=False)}

## Task
Compress and consolidate the experience_library above. Output ONLY the valid JSON object.
"""
        return system_prompt, user_prompt

    def extract_and_update(
        self,
        instance_name: str,
        current_entry: dict[str, Any],
        source_entries: list[dict[str, Any]] | None = None,
        best_entry: dict[str, Any] | None = None,
        problem_description: str | None = None,
        language: str | None = None,
        optimization_target: str | None = None,
    ) -> None:
        """
        æ ¹æ®ä¸€æ¬¡è¿­ä»£çš„æ€»ç»“ä¸æ€§èƒ½æ•°æ®ï¼Œè¿›è¡Œè®°å¿†æç‚¼å¹¶æ›´æ–°æœ¬åœ°è®°å¿†åº“ã€‚

        Args:
            instance_name: å®ä¾‹åç§°ã€‚
            current_entry: å½“å‰è½¨è¿¹æ¡ç›®ï¼ˆåŒ…å« iteration, summary, code, perf_metrics ç­‰ï¼‰ã€‚
            source_entries: æ¥æºè½¨è¿¹æ¡ç›®åˆ—è¡¨ï¼ˆç”¨äºå¯¹æ¯” diff å’Œæ€§èƒ½å˜åŒ–ï¼‰ã€‚
            best_entry: å½“å‰æœ€ä½³è½¨è¿¹æ¡ç›®ï¼ˆç”¨äºå‚è€ƒï¼‰ã€‚
            problem_description: é—®é¢˜æè¿°ã€‚
            language: ç¼–ç¨‹è¯­è¨€ã€‚
            optimization_target: ä¼˜åŒ–ç›®æ ‡ï¼ˆå¦‚ Runtime, Memory ç­‰ï¼‰ã€‚
        """
        memory = self.load()
        attempted = memory.get("direction_board") or []

        # Extract data from entries
        iteration = int(current_entry.get("iteration") or 0)
        perf_metrics = current_entry.get("perf_metrics")
        current_label = str(current_entry.get("label") or "")

        # è®¡ç®—æ€§èƒ½å·®å¼‚ï¼ˆold vs newï¼‰
        perf_old = None
        perf_new = None
        try:
            # New performance
            if perf_metrics:
                new_perf_val = perf_metrics.get("performance")
                perf_new = float(new_perf_val) if new_perf_val is not None else None
            if perf_new is None:
                # Fallback to top-level performance field
                new_perf_val = current_entry.get("performance")
                perf_new = float(new_perf_val) if new_perf_val is not None else None

            # Old performance: Compare against ALL source entries (Best/Min)
            source_perfs = []
            if source_entries:
                for entry in source_entries:
                    val = None
                    # Try perf_metrics
                    entry_perf_metrics = entry.get("perf_metrics")
                    if entry_perf_metrics:
                        perf_val = entry_perf_metrics.get("performance")
                        val = float(perf_val) if perf_val is not None else None
                    # Try top-level
                    if val is None:
                        perf_val = entry.get("performance")
                        val = float(perf_val) if perf_val is not None else None

                    if val is not None:
                        source_perfs.append(val)

            if source_perfs:
                # Assuming that Lower is Better, so we take the minimum of source entries
                perf_old = min(source_perfs)
        except Exception:
            pass

        # LLM æç‚¼ï¼šç”Ÿæˆ Direction Item + ç”Ÿæˆ Reasoning Item
        dir_items: list[dict[str, Any]] = []
        mem_items: list[dict[str, Any]] = []
        if self.llm_client:
            try:
                sys_prompt, user_prompt = self._build_extraction_prompts(
                    problem_description,
                    perf_old,
                    perf_new,
                    source_entries,
                    current_entry,
                    best_entry,
                    attempted,
                    language=language,
                    optimization_target=optimization_target,
                    current_solution_id=current_label,
                )
                last_error: str | None = None
                for attempt in range(1, 4):
                    try:
                        llm_response = self.llm_client.call_with_system_prompt(
                            system_prompt=sys_prompt,
                            user_prompt=user_prompt,
                            temperature=0.7,
                            max_tokens=None,  # ä½¿ç”¨é…ç½®ä¸­çš„ max_output_tokens
                            usage_context="local_memory.extract_and_update",
                        )
                        self.logger.debug(f"LLMåŸå§‹å“åº” (ç¬¬{attempt}æ¬¡):\n{llm_response}")
                        llm_response = self.llm_client.clean_think_tags(llm_response)
                        self.logger.debug(f"LLMæ¸…ç†åå“åº” (ç¬¬{attempt}æ¬¡):\n{llm_response}")
                        parsed_response = self._parse_llm_json(llm_response)
                        self._validate_memory_response(parsed_response)
                        dir_items = [d for d in parsed_response.get("new_direction_items") or [] if isinstance(d, dict)]
                        mem_items = [m for m in parsed_response.get("new_memory_items") or [] if isinstance(m, dict)]
                        # åˆå¹¶å…¨éƒ¨æ–°é¡¹åˆ°å†…å­˜ç»“æ„
                        if dir_items:
                            self._merge_direction_board(memory, dir_items)
                        if mem_items:
                            self._merge_experience_library(memory, mem_items)
                        self.logger.info(f"LLMè®°å¿†æç‚¼æˆåŠŸ (ç¬¬{attempt}æ¬¡)")
                        break
                    except ValueError as e:
                        last_error = "invalid_response_format"
                        self.logger.warning(f"LLMè®°å¿†æç‚¼è§£æå¤±è´¥: å“åº”æ ¼å¼é”™è¯¯æˆ–æ— æœ‰æ•ˆJSONç‰‡æ®µ (ç¬¬{attempt}æ¬¡): {e}")
                    except Exception as e:
                        last_error = "llm_call_failed"
                        self.logger.warning(f"LLMè®°å¿†æç‚¼è°ƒç”¨å¤±è´¥ (ç¬¬{attempt}æ¬¡): {e}")
                if last_error:
                    self.logger.error(f"LLMè®°å¿†æç‚¼æœ€ç»ˆå¤±è´¥: {last_error}")
            except Exception as e:
                self.logger.warning(f"LLMè®°å¿†æç‚¼å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™å›é€€: {e}")

        # ä¸å†è¿›è¡Œå•é¡¹æ’å…¥çš„å…¼å®¹å¤„ç†

        # æ›´æ–°å…¨å±€çŠ¶æ€
        gs = memory.get("global_status") or {}
        gs["current_generation"] = int(iteration)
        try:
            current_solution_id = current_entry.get("label", "")
        except Exception:
            current_solution_id = None
        gs["current_solution_id"] = current_solution_id

        try:
            best_solution_id = best_entry.get("label", "")
        except Exception:
            best_solution_id = None
        gs["best_solution_id"] = best_solution_id

        memory["global_status"] = gs

        # å‹ç¼©ï¼ˆå¿…è¦æ—¶ï¼‰å¹¶ä¿å­˜
        self.compress_if_needed(memory)
        self.save(memory)
        self.logger.info(
            json.dumps(
                {
                    "memory_update": {
                        "instance": instance_name,
                        "iteration": iteration,
                        "label": current_label,
                        "current_generation": memory.get("global_status", {}).get("current_generation"),
                    }
                },
                ensure_ascii=False,
            )
        )
