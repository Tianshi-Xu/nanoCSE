#!/usr/bin/env python3
"""
LLMå®¢æˆ·ç«¯æ¨¡å—
ä¸ºSEæ¡†æ¶æä¾›ç»Ÿä¸€çš„LLMè°ƒç”¨æ¥å£
"""

import json
import os
import re
import threading
import time
import random
from typing import Any  # noqa: UP035

from openai import OpenAI

try:
    from openai import APIError, RateLimitError, APITimeoutError, APIConnectionError, BadRequestError
except Exception:
    APIError = Exception
    RateLimitError = Exception
    APITimeoutError = Exception
    APIConnectionError = Exception
    BadRequestError = Exception

from core.utils.se_logger import get_se_logger


class LLMClient:
    """LLMå®¢æˆ·ç«¯ï¼Œæ”¯æŒå¤šç§æ¨¡å‹å’ŒAPIç«¯ç‚¹"""

    def __init__(self, model_config: dict[str, Any]):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯

        Args:
            model_config: æ¨¡å‹é…ç½®å­—å…¸ï¼ŒåŒ…å«name, api_base, api_keyç­‰
        """
        self.config = model_config
        self.logger = get_se_logger("llm_client", emoji="ğŸ¤–")
        self.token_log_path = os.getenv("SE_TOKEN_LOG_PATH")
        self._token_lock = threading.Lock()
        self.io_log_path = os.getenv("SE_LLM_IO_LOG_PATH")
        self._io_lock = threading.Lock()

        # éªŒè¯å¿…éœ€çš„é…ç½®å‚æ•°
        required_keys = ["name", "api_base", "api_key"]
        missing_keys = [key for key in required_keys if key not in model_config]
        if missing_keys:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„é…ç½®å‚æ•°: {missing_keys}")

        # è¯·æ±‚æ§åˆ¶å‚æ•°ï¼ˆå¸¦é»˜è®¤å€¼ï¼‰
        self.request_timeout: float = float(self.config.get("request_timeout", 600.0))
        self.max_retries: int = int(self.config.get("max_retries", 10))
        self.retry_delay: float = float(self.config.get("retry_delay", 1.5))
        self.retry_backoff_factor: float = float(self.config.get("retry_backoff_factor", 2.0))
        self.retry_jitter: float = float(self.config.get("retry_jitter", 0.3))

        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼Œéµå¾ªapi_test.pyçš„å·¥ä½œæ¨¡å¼ï¼Œå¹¶è®¾ç½®è¶…æ—¶
        self.client = OpenAI(
            api_key=self.config["api_key"],
            base_url=self.config["api_base"],
            timeout=self.request_timeout,
        )

        self.logger.info(f"åˆå§‹åŒ–LLMå®¢æˆ·ç«¯: {self.config['name']}")

    def _is_retryable_error(self, e: Exception) -> bool:
        if isinstance(e, (RateLimitError, APITimeoutError, APIConnectionError)):
            return True
        if isinstance(e, APIError):
            status = getattr(e, "status_code", None) or getattr(e, "status", None)
            if isinstance(status, int) and status in (408, 429, 500, 502, 503, 504):
                return True
        msg = str(e).lower()
        if any(x in msg for x in ("rate limit", "timed out", "timeout", "temporarily unavailable", "connection")):
            return True
        if isinstance(e, BadRequestError):
            return False
        return False

    def _compute_sleep(self, attempt_index: int) -> float:
        base = self.retry_delay * (self.retry_backoff_factor ** max(0, attempt_index - 1))
        jitter = random.uniform(0, self.retry_jitter)
        return base + jitter

    def clean_think_tags(self, text: str) -> str:
        """ç§»é™¤ <think>...</think> æ ‡ç­¾åŠå…¶ä¸­å†…å®¹ï¼Œå¹¶å»é™¤é¦–å°¾ç©ºç™½"""
        try:
            return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL | re.IGNORECASE).strip()
        except Exception:
            return text

    def call_llm(
        self,
        messages: list[dict[str, str]],
        temperature: float = 0.3,
        max_tokens: int | None = None,
        enable_thinking: bool | None = None,
        usage_context: str | None = None,
    ) -> str:
        """
        è°ƒç”¨LLMå¹¶è¿”å›å“åº”å†…å®¹

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ¯ä¸ªæ¶ˆæ¯åŒ…å«roleå’Œcontent
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è¾“å‡ºéšæœºæ€§
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é…ç½®é»˜è®¤å€¼

        Returns:
            LLMå“åº”çš„æ–‡æœ¬å†…å®¹

        Raises:
            Exception: LLMè°ƒç”¨å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        # ä½¿ç”¨é…ç½®ä¸­çš„ max_output_tokens ä½œä¸ºé»˜è®¤å€¼
        if max_tokens is None:
            max_tokens = self.config.get("max_output_tokens", 4000)

        attempt = 0
        last_err: Exception | None = None
        while attempt < self.max_retries:
            try:
                self.logger.debug(f"è°ƒç”¨LLM: {len(messages)} æ¡æ¶ˆæ¯, temp={temperature}, max_tokens={max_tokens}")

                # è§„èŒƒåŒ–æ¨¡å‹åï¼šä»…ç§»é™¤ openai/ å‰ç¼€ï¼Œå…¶ä»–ä¿æŒåŸæ ·
                raw_name = self.config.get("name", "")
                if isinstance(raw_name, str) and raw_name.startswith("openai/"):
                    model_name = raw_name.split("/", 1)[1]
                else:
                    model_name = raw_name

                # ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯è°ƒç”¨ï¼Œä¼ å…¥å¿…éœ€çš„å‚æ•°
                # ç”Ÿæˆå¯é€‰çš„ extra_bodyï¼Œç”¨äºæ§åˆ¶æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡æ¿
                extra_body: dict[str, Any] = {}
                if enable_thinking is not None:
                    extra_body = {"chat_template_kwargs": {"enable_thinking": bool(enable_thinking)}}

                response = self.client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    **({"extra_body": extra_body} if extra_body else {}),
                )

                # æå–å“åº”å†…å®¹
                content = response.choices[0].message.content

                # è®°å½•ä½¿ç”¨æƒ…å†µ
                if getattr(response, "usage", None):
                    self.logger.debug(
                        f"Tokenä½¿ç”¨: è¾“å…¥={getattr(response.usage, 'prompt_tokens', 'æœªçŸ¥')}, "
                        f"è¾“å‡º={getattr(response.usage, 'completion_tokens', 'æœªçŸ¥')}, "
                        f"æ€»è®¡={getattr(response.usage, 'total_tokens', 'æœªçŸ¥')}"
                    )
                    try:
                        if self.token_log_path:
                            entry = {
                                "ts": time.time(),
                                "context": usage_context or "se_perf",
                                "model": model_name,
                                "prompt_tokens": getattr(response.usage, "prompt_tokens", None),
                                "completion_tokens": getattr(response.usage, "completion_tokens", None),
                                "total_tokens": getattr(response.usage, "total_tokens", None),
                                "messages_chars": sum(
                                    len(str(m.get("content", ""))) for m in messages if isinstance(m, dict)
                                ),
                            }
                            iter_env = os.getenv("SE_ITERATION_INDEX")
                            if iter_env is not None:
                                try:
                                    entry["iteration_index"] = int(iter_env)
                                except Exception:
                                    entry["iteration_index"] = iter_env
                            with self._token_lock:
                                with open(self.token_log_path, "a", encoding="utf-8") as f:
                                    f.write(json.dumps(entry, ensure_ascii=False) + "\n")
                    except Exception:
                        pass

                try:
                    if self.io_log_path:
                        io_entry = {
                            "ts": time.time(),
                            "context": usage_context or "se_perf",
                            "model": model_name,
                            "temperature": temperature,
                            "max_tokens": max_tokens,
                            "attempt_index": attempt,
                            "messages": messages,
                            "response": content,
                        }
                        iter_env = os.getenv("SE_ITERATION_INDEX")
                        if iter_env is not None:
                            try:
                                io_entry["iteration_index"] = int(iter_env)
                            except Exception:
                                io_entry["iteration_index"] = iter_env
                        if getattr(response, "usage", None):
                            io_entry["usage"] = {
                                "prompt_tokens": getattr(response.usage, "prompt_tokens", None),
                                "completion_tokens": getattr(response.usage, "completion_tokens", None),
                                "total_tokens": getattr(response.usage, "total_tokens", None),
                            }
                        with self._io_lock:
                            with open(self.io_log_path, "a", encoding="utf-8") as f:
                                f.write(json.dumps(io_entry, ensure_ascii=False) + "\n")
                except Exception:
                    pass

                return content

            except Exception as e:
                last_err = e
                attempt += 1
                should_retry = attempt < self.max_retries and self._is_retryable_error(e)
                self.logger.warning(f"LLMè°ƒç”¨å¤±è´¥: {e}; attempt={attempt}/{self.max_retries}; retry={should_retry}")
                if should_retry:
                    time.sleep(self._compute_sleep(attempt))
                else:
                    break

        assert last_err is not None
        raise last_err

    def call_with_system_prompt(
        self,
        system_prompt: str,
        user_prompt: str,
        temperature: float = 0.3,
        max_tokens: int | None = None,
        usage_context: str | None = None,
    ) -> str:
        """
        ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·æç¤ºè¯è°ƒç”¨LLM

        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°

        Returns:
            LLMå“åº”çš„æ–‡æœ¬å†…å®¹
        """
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]

        return self.call_llm(messages, temperature, max_tokens, usage_context=usage_context)

    @classmethod
    def from_se_config(cls, se_config: dict[str, Any], use_operator_model: bool = False) -> "LLMClient":
        """
        ä»SEæ¡†æ¶é…ç½®åˆ›å»ºLLMå®¢æˆ·ç«¯

        Args:
            se_config: SEæ¡†æ¶é…ç½®å­—å…¸
            use_operator_model: æ˜¯å¦ä½¿ç”¨operator_modelsé…ç½®è€Œä¸æ˜¯ä¸»æ¨¡å‹é…ç½®

        Returns:
            LLMå®¢æˆ·ç«¯å®ä¾‹
        """
        if use_operator_model and "operator_models" in se_config:
            model_config = se_config["operator_models"]
        else:
            model_config = se_config["model"]

        return cls(model_config)


class TrajectorySummarizer:
    """ä¸“é—¨ç”¨äºè½¨è¿¹æ€»ç»“çš„LLMå®¢æˆ·ç«¯åŒ…è£…å™¨"""

    def __init__(self, llm_client: LLMClient, prompt_config: dict[str, Any] | None = None):
        """
        åˆå§‹åŒ–è½¨è¿¹æ€»ç»“å™¨

        Args:
            llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
        """
        self.llm_client = llm_client
        self.logger = get_se_logger("traj_summarizer", emoji="ğŸ“Š")
        self.prompt_config = prompt_config or {}

    def summarize_trajectory(
        self,
        trajectory_content: str,
        patch_content: str,
        iteration: int,
        problem_description: str | None = None,
        best_solution_text: str | None = None,
        target_solution_text: str | None = None,
    ) -> dict[str, Any]:
        """
        ä½¿ç”¨LLMæ€»ç»“è½¨è¿¹å†…å®¹

        Args:
            trajectory_content: .traæ–‡ä»¶å†…å®¹
            patch_content: .patch/.predæ–‡ä»¶å†…å®¹ (é¢„æµ‹ç»“æœ)
            iteration: è¿­ä»£æ¬¡æ•°
            problem_description: é—®é¢˜æè¿°ï¼ˆå¯é€‰ï¼Œå°†å¹¶å…¥æç¤ºè¯ï¼‰

        Returns:
            è½¨è¿¹æ€»ç»“å­—å…¸
        """
        from .traj_summarizer import TrajSummarizer

        summarizer = TrajSummarizer(self.prompt_config)

        # è·å–æç¤ºè¯
        system_prompt = summarizer.get_system_prompt()
        user_prompt = summarizer.format_user_prompt(
            trajectory_content,
            patch_content,
            problem_description,
            best_solution=best_solution_text,
            target_solution=target_solution_text,
        )

        self.logger.info(f"å¼€å§‹LLMè½¨è¿¹æ€»ç»“ (è¿­ä»£{iteration})")
        self.logger.debug(f"LLMç³»ç»Ÿæç¤ºè¯ (è¿­ä»£{iteration}):\n{system_prompt}")
        self.logger.debug(f"LLMç”¨æˆ·æç¤ºè¯ (è¿­ä»£{iteration}):\n{user_prompt}")

        # é‡è¯•æœºåˆ¶ï¼šè§£æå¤±è´¥æˆ–è°ƒç”¨å¤±è´¥æ—¶é‡è¯•ï¼Œæ€»æ¬¡æ•°3æ¬¡
        last_error: str | None = None
        for attempt in range(1, 4):
            try:
                response = self.llm_client.call_with_system_prompt(
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                    temperature=0.7,
                    max_tokens=None,  # ä½¿ç”¨é…ç½®ä¸­çš„ max_output_tokens
                    usage_context="traj_summarizer",
                )
                self.logger.debug(f"LLMåŸå§‹å“åº” (è¿­ä»£{iteration}, ç¬¬{attempt}æ¬¡):\n{response}")
                # å»é™¤æ€è€ƒå†…å®¹
                response = self.llm_client.clean_think_tags(response)
                self.logger.debug(f"LLMæ¸…ç†åå“åº” (è¿­ä»£{iteration}, ç¬¬{attempt}æ¬¡):\n{response}")

                summary = summarizer.parse_response(response)
                summarizer.validate_response_format(summary)

                self.logger.info(f"LLMè½¨è¿¹æ€»ç»“æˆåŠŸ (è¿­ä»£{iteration}, ç¬¬{attempt}æ¬¡)")
                return summary

            except json.JSONDecodeError as e:
                last_error = "json_decode_error"
                self.logger.warning(f"LLMè½¨è¿¹æ€»ç»“è§£æå¤±è´¥: JSONè§£æé”™è¯¯ (è¿­ä»£{iteration}, ç¬¬{attempt}æ¬¡): {e}")
            except ValueError as e:
                last_error = "invalid_response_format"
                self.logger.warning(
                    f"LLMè½¨è¿¹æ€»ç»“è§£æå¤±è´¥: å“åº”æ ¼å¼é”™è¯¯æˆ–æ— æœ‰æ•ˆJSONç‰‡æ®µ (è¿­ä»£{iteration}, ç¬¬{attempt}æ¬¡): {e}"
                )
            except Exception as e:
                last_error = "llm_call_failed"
                self.logger.warning(f"LLMè½¨è¿¹æ€»ç»“è°ƒç”¨å¤±è´¥ (è¿­ä»£{iteration}, ç¬¬{attempt}æ¬¡): {e}")

        # æ‰€æœ‰é‡è¯•å¤±è´¥ï¼Œè¿”å›å¤‡ç”¨æ€»ç»“
        if last_error:
            self.logger.error(f"LLMè½¨è¿¹æ€»ç»“æœ€ç»ˆå¤±è´¥ (è¿­ä»£{iteration}): {last_error}")
        return summarizer.create_fallback_summary(trajectory_content, patch_content, iteration)
