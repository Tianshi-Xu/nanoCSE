"""
LLM å®¢æˆ·ç«¯ï¼Œæ”¯æŒå¤šç§æ¨¡å‹å’ŒAPIç«¯ç‚¹
"""

import json
import logging
import os
import random
import threading
import time
from pathlib import Path
from typing import Any

from openai import OpenAI

try:
    from openai import APIConnectionError, APIError, APITimeoutError, BadRequestError, RateLimitError
except Exception:
    APIError = Exception
    RateLimitError = Exception
    APITimeoutError = Exception
    APIConnectionError = Exception
    BadRequestError = Exception

from .utils.log import get_se_logger


class LLMClient:
    """LLMå®¢æˆ·ç«¯ï¼Œæ”¯æŒå¤šç§æ¨¡å‹å’ŒAPIç«¯ç‚¹"""

    def __init__(
        self,
        model_config: dict[str, Any],
        max_retries: int = 10,
        retry_delay: float = 1.5,
        retry_backoff_factor: float = 2.0,
        retry_jitter: float = 0.3,
        io_log_path: str | Path | None = None,
        log_inputs_outputs: bool = True,
        log_sanitize: bool = True,
        request_timeout: float = 60.0,
    ):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯

        Args:
            model_config: æ¨¡å‹é…ç½®å­—å…¸ï¼ŒåŒ…å«name, api_base, api_keyç­‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay: æ¯æ¬¡é‡è¯•çš„ç­‰å¾…ç§’æ•°
            io_log_path: LLM è¾“å…¥/è¾“å‡ºæ—¥å¿—æ–‡ä»¶è·¯å¾„
            log_inputs_outputs: æ˜¯å¦è®°å½•åŸå§‹è¾“å…¥è¾“å‡º
        """
        self.config = model_config
        # ç»Ÿä¸€ä½¿ç”¨æ–‡ä»¶æ—¥å¿—ï¼ˆå¸¦ emojiï¼‰ï¼Œä¸ IO æ—¥å¿—åŒç›®å½•
        self.io_log_path = Path(io_log_path) if io_log_path else Path("./logs/llm_io.log")
        # Logger åç§°å¢åŠ ä»»åŠ¡ååç¼€ï¼ˆå–æ—¥å¿—ç›®å½•åï¼‰ï¼Œé¿å…å¹¶å‘ä»»åŠ¡å†²çª
        task_suffix = self.io_log_path.parent.name or "default"
        logger_name = f"perfagent.llm_client.{task_suffix}"
        get_se_logger(logger_name, self.io_log_path, emoji="ğŸ¤–", also_stream=False)
        self.logger = logging.getLogger(logger_name)
        self.token_log_path = os.getenv("SE_TOKEN_LOG_PATH")
        self._token_lock = threading.Lock()
        self.io_jsonl_path = os.getenv("SE_LLM_IO_LOG_PATH")
        self._io_lock = threading.Lock()

        # ä¼˜å…ˆä½¿ç”¨é…ç½®ä¸­çš„å¢å¼ºå‚æ•°
        self.max_retries = int(model_config.get("max_retries", max_retries))
        self.retry_delay = float(model_config.get("retry_delay", retry_delay))
        self.retry_backoff_factor = float(model_config.get("retry_backoff_factor", retry_backoff_factor))
        self.retry_jitter = float(model_config.get("retry_jitter", retry_jitter))
        self.log_inputs_outputs = bool(model_config.get("log_inputs_outputs", log_inputs_outputs))
        self.log_sanitize = bool(model_config.get("log_sanitize", log_sanitize))
        self.request_timeout = float(model_config.get("request_timeout", request_timeout))

        # éªŒè¯å¿…éœ€çš„é…ç½®å‚æ•°
        required_keys = ["name", "api_base", "api_key"]
        missing_keys = [key for key in required_keys if key not in model_config]
        if missing_keys:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„é…ç½®å‚æ•°: {missing_keys}")

        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼Œéµå¾ªapi_test.pyçš„å·¥ä½œæ¨¡å¼
        self.client = OpenAI(
            api_key=self.config["api_key"],
            base_url=self.config["api_base"],
            timeout=self.request_timeout,
        )

        self.logger.info(f"åˆå§‹åŒ–LLMå®¢æˆ·ç«¯: {self.config['name']}")

    def _is_retryable_error(self, e: Exception) -> bool:
        if isinstance(e, (RateLimitError, APITimeoutError, APIConnectionError)):
            return True
        if isinstance(e, APIError):
            status = getattr(e, "status_code", None) or getattr(e, "status", None)
            if isinstance(status, int) and status in (408, 429, 500, 502, 503, 504):
                return True
        msg = str(e).lower()
        if any(x in msg for x in ("rate limit", "timed out", "timeout", "temporarily unavailable", "connection")):
            return True
        if isinstance(e, BadRequestError):
            return False
        return False

    def _compute_sleep(self, attempt_index: int) -> float:
        base = self.retry_delay * (self.retry_backoff_factor ** max(0, attempt_index - 1))
        jitter = random.uniform(0, self.retry_jitter)
        return base + jitter

    def _format_content_for_log(self, content: str | None, indent: int = 2) -> str:
        """å°†æ–‡æœ¬å†…å®¹æ ¼å¼åŒ–ä¸ºå¤šè¡Œæ—¥å¿—ï¼Œä¿ç•™çœŸå®æ¢è¡Œå¹¶ç¼©è¿›ã€‚

        Args:
            content: æ–‡æœ¬å†…å®¹
            indent: ç¼©è¿›ç©ºæ ¼æ•°é‡

        Returns:
            å‹å¥½çš„å¤šè¡Œå­—ç¬¦ä¸²ï¼Œå¸¦ç¼©è¿›å¹¶ä¿ç•™æ¢è¡Œ
        """
        prefix = " " * indent
        if content is None:
            return f"{prefix}content: (None)"
        text = str(content)
        if text == "":
            return f"{prefix}content: (empty)"
        lines = text.splitlines() or [text]
        formatted = [f"{prefix}content:"]
        formatted.extend(f"{prefix}  {line}" for line in lines)
        return "\n".join(formatted)

    def _format_messages_for_log(self, messages: list[dict[str, str]], indent: int = 0) -> str:
        """å°†æ¶ˆæ¯åˆ—è¡¨æ ¼å¼åŒ–ä¸ºå¤šè¡Œæ—¥å¿—ï¼Œä¿ç•™çœŸå®æ¢è¡Œå¹¶ç¼©è¿›å†…å®¹ã€‚"""
        base_prefix = " " * indent
        out_lines = [f"{base_prefix}messages:"]
        for i, m in enumerate(messages, start=1):
            role = m.get("role", "unknown")
            out_lines.append(f"{base_prefix}  [{i}] role: {role}")
            out_lines.append(self._format_content_for_log(m.get("content"), indent=indent + 4))
        return "\n".join(out_lines)

    def call_llm(
        self,
        messages: list[dict[str, str]],
        temperature: float = 0.3,
        max_tokens: int | None = None,
        usage_context: str | None = None,
    ) -> str:
        """
        è°ƒç”¨LLMå¹¶è¿”å›å“åº”å†…å®¹

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ¯ä¸ªæ¶ˆæ¯åŒ…å«roleå’Œcontent
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è¾“å‡ºéšæœºæ€§
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é…ç½®é»˜è®¤å€¼

        Returns:
            LLMå“åº”çš„æ–‡æœ¬å†…å®¹

        Raises:
            Exception: LLMè°ƒç”¨å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        from datetime import datetime

        # ä½¿ç”¨é…ç½®ä¸­çš„max_output_tokensä½œä¸ºé»˜è®¤å€¼
        if max_tokens is None:
            max_tokens = self.config.get("max_output_tokens", 4000)

        call_start_time = time.time()
        attempt = 0
        total_retry_wait_time = 0.0
        rate_limit_count = 0
        timeout_count = 0
        connection_error_count = 0
        last_err: Exception | None = None

        self.logger.info(
            f"[LLMè¯·æ±‚å¼€å§‹] æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}, "
            f"æ¶ˆæ¯æ•°: {len(messages)}, max_tokens: {max_tokens}, context: {usage_context or 'default'}"
        )

        while attempt < self.max_retries:
            try:
                self.logger.debug(f"è°ƒç”¨LLM: {len(messages)} æ¡æ¶ˆæ¯, temp={temperature}, max_tokens={max_tokens}")

                # è®°å½•åŸå§‹è¾“å…¥
                if self.log_inputs_outputs:
                    try:
                        # æ—¥å¿—è„±æ•ï¼šç§»é™¤å¯èƒ½çš„å¯†é’¥ä¸ç«¯ç‚¹ä¿¡æ¯
                        model_name = self.config.get("name", "unknown")
                        api_base = self.config.get("api_base", "")
                        safe_api_base = "<redacted>" if self.log_sanitize and api_base else api_base
                        req_lines = [
                            "LLM Request:",
                            f"model: {model_name}",
                            f"temperature: {temperature}",
                            f"max_tokens: {max_tokens}",
                            f"api_base: {safe_api_base}",
                            self._format_messages_for_log(messages, indent=0),
                        ]
                        self.logger.info("\n".join(req_lines))
                    except Exception as log_e:
                        self.logger.error(f"è®°å½•è¯·æ±‚å¤±è´¥: {log_e}")

                # ä½¿ç”¨åŸºæœ¬çš„OpenAIå®¢æˆ·ç«¯è°ƒç”¨ï¼Œéµå¾ªapi_test.pyçš„å·¥ä½œæ¨¡å¼
                # ä¸ä½¿ç”¨é¢å¤–å‚æ•°ï¼Œé¿å…æœåŠ¡å™¨é”™è¯¯
                model_to_use = "/".join(self.config["name"].split("/")[1:])
                self.logger.debug(f"è°ƒç”¨æ¨¡å‹: {model_to_use}, max_tokens={max_tokens}")

                response = self.client.chat.completions.create(
                    model=model_to_use,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                )

                # æ£€æŸ¥å“åº”æ˜¯å¦æœ‰æ•ˆ - ç©º choices è§†ä¸ºå¯é‡è¯•é”™è¯¯
                if not response.choices:
                    self.logger.warning(
                        f"APIè¿”å›ç©ºchoices (å°è¯• {attempt + 1}/{self.max_retries}), Response: {response}"
                    )
                    # ç©ºå“åº”æ—¶è¿›è¡Œé‡è¯•
                    attempt += 1  # é€’å¢é‡è¯•è®¡æ•°
                    if attempt < self.max_retries:
                        time.sleep(self.retry_delay * attempt)
                        continue
                    else:
                        raise ValueError(
                            f"APIè¿”å›ç©ºchoicesï¼Œå·²é‡è¯•{self.max_retries}æ¬¡ã€‚Response id: {response.id}, model: {response.model}"
                        )

                # æå–å“åº”å†…å®¹
                content = response.choices[0].message.content

                # è®°å½•ä½¿ç”¨æƒ…å†µ
                if getattr(response, "usage", None):
                    self.logger.debug(
                        f"Tokenä½¿ç”¨: è¾“å…¥={getattr(response.usage, 'prompt_tokens', 'æœªçŸ¥')}, "
                        f"è¾“å‡º={getattr(response.usage, 'completion_tokens', 'æœªçŸ¥')}, "
                        f"æ€»è®¡={getattr(response.usage, 'total_tokens', 'æœªçŸ¥')}"
                    )
                    try:
                        if self.token_log_path:
                            entry = {
                                "ts": time.time(),
                                "context": usage_context or "perfagent",
                                "model": "/".join(self.config["name"].split("/")[1:]),
                                "prompt_tokens": getattr(response.usage, "prompt_tokens", None),
                                "completion_tokens": getattr(response.usage, "completion_tokens", None),
                                "total_tokens": getattr(response.usage, "total_tokens", None),
                                "messages_chars": sum(
                                    len(str(m.get("content", ""))) for m in messages if isinstance(m, dict)
                                ),
                            }
                            iter_env = os.getenv("SE_ITERATION_INDEX")
                            if iter_env is not None:
                                try:
                                    entry["iteration_index"] = int(iter_env)
                                except Exception:
                                    entry["iteration_index"] = iter_env
                            with self._token_lock:
                                with open(self.token_log_path, "a", encoding="utf-8") as f:
                                    f.write(json.dumps(entry, ensure_ascii=False) + "\n")
                    except Exception:
                        pass

                try:
                    if self.io_jsonl_path:
                        io_entry = {
                            "ts": time.time(),
                            "context": usage_context or "perfagent",
                            "model": "/".join(self.config["name"].split("/")[1:]),
                            "temperature": temperature,
                            "max_tokens": max_tokens,
                            "attempt_index": attempt,
                            "messages": messages,
                            "response": content,
                        }
                        iter_env = os.getenv("SE_ITERATION_INDEX")
                        if iter_env is not None:
                            try:
                                io_entry["iteration_index"] = int(iter_env)
                            except Exception:
                                io_entry["iteration_index"] = iter_env
                        if getattr(response, "usage", None):
                            io_entry["usage"] = {
                                "prompt_tokens": getattr(response.usage, "prompt_tokens", None),
                                "completion_tokens": getattr(response.usage, "completion_tokens", None),
                                "total_tokens": getattr(response.usage, "total_tokens", None),
                            }
                        with self._io_lock:
                            with open(self.io_jsonl_path, "a", encoding="utf-8") as f:
                                f.write(json.dumps(io_entry, ensure_ascii=False) + "\n")
                except Exception:
                    pass

                # è®°å½•åŸå§‹è¾“å‡º
                if self.log_inputs_outputs:
                    try:
                        usage = getattr(response, "usage", None)
                        usage_dict = None
                        if usage:
                            usage_dict = {
                                "prompt_tokens": getattr(usage, "prompt_tokens", None),
                                "completion_tokens": getattr(usage, "completion_tokens", None),
                                "total_tokens": getattr(usage, "total_tokens", None),
                            }
                        # å“åº”æ—¥å¿—è„±æ•ï¼šä¸è®°å½•åŸå§‹å¤´ä¿¡æ¯
                        resp_lines = [
                            "LLM Response:",
                            self._format_content_for_log(content, indent=0),
                            "usage: " + json.dumps(usage_dict, ensure_ascii=False),
                        ]
                        self.logger.info("\n".join(resp_lines))
                    except Exception as log_e:
                        self.logger.error(f"è®°å½•å“åº”å¤±è´¥: {log_e}")

                # è®°å½•æˆåŠŸè°ƒç”¨çš„ç»Ÿè®¡ä¿¡æ¯
                total_elapsed = time.time() - call_start_time
                usage = getattr(response, "usage", None)
                prompt_tokens = getattr(usage, "prompt_tokens", 0) if usage else 0
                completion_tokens = getattr(usage, "completion_tokens", 0) if usage else 0

                self.logger.info(
                    f"[LLMè¯·æ±‚æˆåŠŸ] æ€»è€—æ—¶: {total_elapsed:.2f}s ({total_elapsed / 60:.1f}åˆ†é’Ÿ)\n"
                    f"  - å°è¯•æ¬¡æ•°: {attempt + 1}\n"
                    f"  - é‡è¯•ç­‰å¾…æ—¶é—´: {total_retry_wait_time:.2f}s\n"
                    f"  - å®é™…APIè€—æ—¶: {total_elapsed - total_retry_wait_time:.2f}s\n"
                    f"  - é™æµæ¬¡æ•°: {rate_limit_count}\n"
                    f"  - è¶…æ—¶æ¬¡æ•°: {timeout_count}\n"
                    f"  - è¿æ¥é”™è¯¯æ¬¡æ•°: {connection_error_count}\n"
                    f"  - Tokenä½¿ç”¨: prompt={prompt_tokens}, completion={completion_tokens}, "
                    f"total={prompt_tokens + completion_tokens}\n"
                    f"  - å“åº”é•¿åº¦: {len(content) if content else 0} å­—ç¬¦"
                )

                return content

            except Exception as e:
                last_err = e
                attempt += 1
                should_retry = attempt < self.max_retries and self._is_retryable_error(e)

                # åˆ†ç±»é”™è¯¯ç±»å‹
                error_type = type(e).__name__
                if isinstance(e, RateLimitError) or "rate limit" in str(e).lower():
                    rate_limit_count += 1
                    error_category = "é™æµ(RateLimit)"
                elif isinstance(e, APITimeoutError) or "timeout" in str(e).lower():
                    timeout_count += 1
                    error_category = "è¶…æ—¶(Timeout)"
                elif isinstance(e, APIConnectionError) or "connection" in str(e).lower():
                    connection_error_count += 1
                    error_category = "è¿æ¥é”™è¯¯(Connection)"
                else:
                    error_category = "å…¶ä»–é”™è¯¯"

                elapsed_so_far = time.time() - call_start_time
                sleep_time = self._compute_sleep(attempt) if should_retry else 0

                try:
                    self.logger.warning(
                        f"[LLMè°ƒç”¨å¤±è´¥] ç¬¬ {attempt}/{self.max_retries} æ¬¡å°è¯•\n"
                        f"  - é”™è¯¯ç±»åˆ«: {error_category}\n"
                        f"  - é”™è¯¯ç±»å‹: {error_type}\n"
                        f"  - é”™è¯¯ä¿¡æ¯: {e}\n"
                        f"  - å·²è€—æ—¶: {elapsed_so_far:.2f}s\n"
                        f"  - å°†é‡è¯•: {'æ˜¯' if should_retry else 'å¦'}\n"
                        f"  - ç­‰å¾…æ—¶é—´: {sleep_time:.2f}s\n"
                        f"  - ç´¯è®¡ç»Ÿè®¡: é™æµ={rate_limit_count}, è¶…æ—¶={timeout_count}, è¿æ¥é”™è¯¯={connection_error_count}"
                    )
                except Exception:
                    pass

                if should_retry:
                    try:
                        time.sleep(sleep_time)
                        total_retry_wait_time += sleep_time
                    except Exception:
                        time.sleep(self.retry_delay)
                        total_retry_wait_time += self.retry_delay
                else:
                    break

        # é‡è¯•åä»å¤±è´¥ï¼ŒæŠ›å‡ºæœ€åä¸€ä¸ªé”™è¯¯
        total_elapsed = time.time() - call_start_time
        self.logger.error(
            f"[LLMè°ƒç”¨æœ€ç»ˆå¤±è´¥] æ€»è€—æ—¶: {total_elapsed:.2f}s\n"
            f"  - å°è¯•æ¬¡æ•°: {attempt}\n"
            f"  - é‡è¯•ç­‰å¾…æ€»æ—¶é—´: {total_retry_wait_time:.2f}s\n"
            f"  - é™æµæ¬¡æ•°: {rate_limit_count}\n"
            f"  - è¶…æ—¶æ¬¡æ•°: {timeout_count}\n"
            f"  - è¿æ¥é”™è¯¯æ¬¡æ•°: {connection_error_count}\n"
            f"  - æœ€åé”™è¯¯: {type(last_err).__name__}: {last_err}"
        )
        assert last_err is not None
        raise last_err

    def call_with_system_prompt(
        self, system_prompt: str, user_prompt: str, temperature: float = 0.3, max_tokens: int | None = None
    ) -> str:
        """
        ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·æç¤ºè¯è°ƒç”¨LLM

        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°

        Returns:
            LLMå“åº”çš„æ–‡æœ¬å†…å®¹
        """
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]

        return self.call_llm(messages, temperature, max_tokens)
