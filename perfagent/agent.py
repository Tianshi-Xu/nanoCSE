"""
PerfAgent æ ¸å¿ƒç±»

å®ç°é€šç”¨çš„è¿­ä»£ä¼˜åŒ–å¾ªç¯ï¼Œé€šè¿‡ TaskRunner æ’ä»¶æ”¯æŒå¤šç§ä»»åŠ¡ç±»å‹ã€‚
Agent ä¸ç›´æ¥å¤„ç†ä»»åŠ¡ç‰¹å®šçš„æ•°æ®ç»“æ„ï¼Œæ‰€æœ‰ä»»åŠ¡ç‰¹å®šæ“ä½œå‡å§”æ‰˜ç»™ TaskRunnerã€‚
"""

import logging
import time
import traceback
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import TYPE_CHECKING, Any

from .config import PerfAgentConfig

if TYPE_CHECKING:
    from .protocols import AgentRequest, AgentResult

from .llm_client import LLMClient
from .task_runner import BaseTaskRunner
from .trajectory import TrajectoryLogger
from .utils.log import get_se_logger

# å‘åå…¼å®¹ re-exportï¼šEffiBenchXInstance å·²è¿ç§»åˆ° perfagent/tasks/effibench.pyï¼Œ
# æ­¤å¤„ re-export ä»¥ä¿æŒæ‰€æœ‰ç°æœ‰ import ç»§ç»­å·¥ä½œã€‚
from .tasks.effibench import EffiBenchXInstance  # noqa: F401


@dataclass
class RunContext:
    """ä¿å­˜å•æ¬¡è¿è¡Œçš„ä¸Šä¸‹æ–‡çŠ¶æ€ï¼ˆé€šç”¨ï¼Œä»»åŠ¡æ— å…³ï¼‰

    Attributes:
        instance_data: ä»»åŠ¡å®ä¾‹æ•°æ®ï¼ˆä¸é€æ˜ï¼Œç”± TaskRunner è§£é‡Šï¼‰
        trajectory: è½¨è¿¹è®°å½•å™¨
        current_solution: å½“å‰è§£
        best_solution: æœ€ä¼˜è§£
        best_metric: æœ€ä¼˜æ ‡é‡æŒ‡æ ‡ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
        current_artifacts: å½“å‰è¯„ä¼°çš„ artifacts
        best_artifacts: æœ€ä¼˜è§£å¯¹åº”çš„ artifacts
        optimization_history: ä¼˜åŒ–å†å²è®°å½•
        no_improve_count: è¿ç»­æœªæ”¹è¿›æ¬¡æ•°
    """

    instance_data: Any
    trajectory: TrajectoryLogger
    current_solution: str
    best_solution: str
    best_metric: float
    current_artifacts: dict[str, Any]
    best_artifacts: dict[str, Any]
    optimization_history: list[dict[str, Any]]
    no_improve_count: int = 0


class PerfAgent:
    """é€šç”¨æ€§èƒ½ä¼˜åŒ– Agent

    é€šè¿‡ TaskRunner æ’ä»¶å®ç°ä»»åŠ¡æ— å…³çš„è¿­ä»£ä¼˜åŒ–å¾ªç¯ã€‚
    ä½¿ç”¨ AgentRequest/AgentResult åè®®ä¸ SE_Perf å±‚é€šä¿¡ã€‚
    """

    def __init__(self, config: PerfAgentConfig, task_runner: BaseTaskRunner | None = None):
        self.config = config
        self.task_runner = task_runner

        # ç®€åŒ–é€»è¾‘ï¼šå‡­æ®å­˜åœ¨å³åˆå§‹åŒ– LLMClientï¼Œæ— éœ€ use_llm æ ‡å¿—
        self.llm_client = None
        if self.config.model.api_base and self.config.model.api_key:
            client_cfg = {
                "name": self.config.model.name,
                "api_base": self.config.model.api_base,
                "api_key": self.config.model.api_key,
                "max_output_tokens": self.config.model.max_output_tokens,
                "request_timeout": self.config.model.request_timeout,
                "max_retries": self.config.model.max_retries,
                "retry_delay": self.config.model.retry_delay,
                "retry_backoff_factor": getattr(self.config.model, "retry_backoff_factor", 2.0),
                "retry_jitter": getattr(self.config.model, "retry_jitter", 0.5),
                "log_inputs_outputs": self.config.model.log_inputs_outputs,
                "log_sanitize": self.config.model.log_sanitize,
            }
            # å°† LLM I/O ç‹¬ç«‹å†™å…¥ log_dir/llm_io.log
            io_log_file = Path(self.config.logging.log_dir) / "llm_io.log"
            self.llm_client = LLMClient(
                client_cfg,
                io_log_path=io_log_file,
                log_inputs_outputs=self.config.model.log_inputs_outputs,
                log_sanitize=self.config.model.log_sanitize,
                request_timeout=self.config.model.request_timeout,
            )

        # è®¾ç½®æ—¥å¿—ï¼šç»Ÿä¸€ç»‘å®šåˆ°å•ä¸€æ–‡ä»¶
        # ä½¿ç”¨åŒ…å«æ—¥å¿—ç›®å½•åçš„å”¯ä¸€ logger åç§°ï¼Œé¿å…å¹¶å‘å®ä¾‹å¤ç”¨åŒåå¯¼è‡´ä¸²å†™
        agent_logger_name = f"perfagent.agent.{Path(self.config.logging.log_dir).name}"
        get_se_logger(
            agent_logger_name,
            Path(self.config.logging.log_dir) / "perfagent.log",
            emoji="ğŸ”§",
            level=getattr(logging, self.config.logging.log_level.upper()),
            also_stream=False,
        )
        self.logger = logging.getLogger(agent_logger_name)

        # ä¼˜åŒ–å†å²
        self.optimization_history: list[dict[str, Any]] = []

    # ==================================================================
    # TaskRunner ç®¡ç†
    # ==================================================================

    def _ensure_task_runner(self) -> BaseTaskRunner:
        """ç¡®ä¿ TaskRunner å·²è®¾ç½®ã€‚è‹¥æ„é€ æ—¶æœªæä¾›ï¼Œè‡ªåŠ¨åˆ›å»º EffiBenchRunnerï¼ˆå‘åå…¼å®¹ï¼‰ã€‚"""
        if self.task_runner is not None:
            return self.task_runner
        from .tasks.effibench import EffiBenchRunner

        self.task_runner = EffiBenchRunner(
            task_config=self.config.task_config,
            _logger=self.logger,
        )
        return self.task_runner

    @staticmethod
    def _get_instance_id(instance_data: Any) -> str:
        """ä»å®ä¾‹æ•°æ®ä¸­æå– IDï¼ˆå°è¯•å¤šä¸ªå±æ€§åï¼‰"""
        for attr in ("task_name", "id", "instance_id"):
            val = getattr(instance_data, attr, None)
            if val:
                return str(val)
        if isinstance(instance_data, dict):
            for key in ("task_name", "id", "instance_id"):
                if key in instance_data:
                    return str(instance_data[key])
        return "unknown"

    # ==================================================================
    # ä¸»å…¥å£
    # ==================================================================

    def run(self, instance_data: Any) -> dict[str, Any]:
        """è¿è¡Œä¼˜åŒ–æµç¨‹ï¼ˆé€šç”¨å…¥å£ï¼‰

        æµç¨‹ï¼šåˆå§‹åŒ–ä¸Šä¸‹æ–‡ -> ä¼˜åŒ–å¾ªç¯ -> ç”Ÿæˆç»“æœ
        ç¬¬ä¸€æ¬¡è¿­ä»£å³ç”Ÿæˆåˆå§‹è§£å¹¶è¯„ä¼°ï¼Œä¸å†æœ‰ç‹¬ç«‹çš„"åˆå§‹è¯„ä¼°"æ­¥éª¤ã€‚

        Args:
            instance_data: ä»»åŠ¡å®ä¾‹æ•°æ®ï¼ˆä¸é€æ˜å¯¹è±¡ï¼Œç”± TaskRunner è§£é‡Šï¼‰ã€‚
                           å‘åå…¼å®¹ï¼šå¯ä¼ å…¥ EffiBenchXInstanceã€‚

        Returns:
            ç»“æœå­—å…¸ï¼Œå…¼å®¹ AgentResult.from_dict
        """
        self._ensure_task_runner()
        run_start_time = time.time()
        instance_id = self._get_instance_id(instance_data)

        self.logger.info(
            f"\n{'#' * 70}\n"
            f"# [PerfAgent è¿è¡Œå¼€å§‹]\n"
            f"# æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            f"# å®ä¾‹: {instance_id}\n"
            f"# æœ€å¤§è¿­ä»£æ¬¡æ•°: {self.config.max_iterations}\n"
            f"# æ¨¡å‹: {self.config.model.name}\n"
            f"{'#' * 70}"
        )

        try:
            # 1. åˆå§‹åŒ–ä¸Šä¸‹æ–‡
            init_start = time.time()
            ctx = self._init_run_context(instance_data)
            init_elapsed = time.time() - init_start
            self.logger.info(f"[ä¸Šä¸‹æ–‡åˆå§‹åŒ–å®Œæˆ] è€—æ—¶: {init_elapsed:.3f}s")

            # 2. ä¼˜åŒ–å¾ªç¯ï¼ˆç¬¬ä¸€æ¬¡è¿­ä»£å³ç”Ÿæˆåˆå§‹è§£å¹¶è¯„ä¼°ï¼‰
            loop_start = time.time()
            self._process_optimization_loop(ctx)
            loop_elapsed = time.time() - loop_start
            self.logger.info(f"[ä¼˜åŒ–å¾ªç¯å®Œæˆ] æ€»è€—æ—¶: {loop_elapsed:.2f}s ({loop_elapsed / 60:.1f}åˆ†é’Ÿ)")

            # 3. å®Œæˆå¹¶ç”Ÿæˆç»“æœ
            result = self._finalize_run(ctx)

            run_elapsed = time.time() - run_start_time
            self.logger.info(
                f"\n{'#' * 70}\n"
                f"# [PerfAgent è¿è¡ŒæˆåŠŸå®Œæˆ]\n"
                f"# å®ä¾‹: {instance_id}\n"
                f"# æ€»è€—æ—¶: {run_elapsed:.2f}s ({run_elapsed / 60:.1f}åˆ†é’Ÿ)\n"
                f"# æˆåŠŸ: {result.get('success', False)}\n"
                f"{'#' * 70}"
            )
            return result

        except Exception as e:
            run_elapsed = time.time() - run_start_time
            self.logger.error(
                f"\n{'!' * 70}\n"
                f"! [PerfAgent è¿è¡Œå¤±è´¥]\n"
                f"! å®ä¾‹: {instance_id}\n"
                f"! è¿è¡Œè€—æ—¶: {run_elapsed:.2f}s ({run_elapsed / 60:.1f}åˆ†é’Ÿ)\n"
                f"! é”™è¯¯ç±»å‹: {type(e).__name__}\n"
                f"! é”™è¯¯ä¿¡æ¯: {e}\n"
                f"! å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}\n"
                f"{'!' * 70}"
            )
            # å°è¯•è®°å½•é”™è¯¯è½¨è¿¹
            try:
                if "ctx" in locals():
                    ctx.trajectory.finalize(
                        success=False, error_message=str(e), final_submission_code=ctx.best_solution
                    )
                    self.logger.info("[è½¨è¿¹å·²ä¿å­˜] é”™è¯¯è½¨è¿¹è®°å½•å®Œæˆ")
            except Exception as traj_error:
                self.logger.warning(f"[è½¨è¿¹ä¿å­˜å¤±è´¥] {type(traj_error).__name__}: {traj_error}")
            raise

    # ==================================================================
    # AgentRequest / AgentResult API
    # ==================================================================

    def run_with_request(self, request: "AgentRequest") -> "AgentResult":
        """æ¥æ”¶ AgentRequestï¼Œè¿”å› AgentResultã€‚

        Args:
            request: AgentRequest å¯¹è±¡

        Returns:
            AgentResult
        """
        from .protocols import AgentRequest, AgentResult

        if not isinstance(request, AgentRequest):
            raise TypeError(f"ä¸æ”¯æŒçš„è¯·æ±‚ç±»å‹: {type(request).__name__}ï¼Œè¯·ä½¿ç”¨ AgentRequest")

        runner = self._ensure_task_runner()

        # åº”ç”¨è¯·æ±‚ä¸­çš„è¦†ç›–å‚æ•°
        if request.additional_requirements:
            self.config.prompts.additional_requirements = request.additional_requirements
        if request.local_memory:
            self.config.prompts.local_memory = request.local_memory
        if request.global_memory:
            self.config.prompts.global_memory = request.global_memory
        if request.output_dir:
            self.config.logging.trajectory_dir = str(request.output_dir)
            self.config.logging.log_dir = str(request.output_dir)

        try:
            # é€šè¿‡ TaskRunner åŠ è½½å®ä¾‹
            instance_data = runner.load_instance(request.task_data_path)
            raw_result = self.run(instance_data)

            return AgentResult(
                instance_id=raw_result.get("instance_id", "unknown"),
                success=raw_result.get("success", False),
                solution=raw_result.get("solution", ""),
                metric=raw_result.get("metric", float("inf")),
                artifacts=raw_result.get("artifacts", {}),
                total_iterations=raw_result.get("total_iterations", 0),
                trajectory_file=raw_result.get("trajectory_file"),
                error=raw_result.get("error"),
            )
        except Exception as e:
            self.logger.error(f"[run_with_request å¼‚å¸¸] {type(e).__name__}: {e}", exc_info=True)
            return AgentResult.from_error(instance_id="unknown", error=str(e))

    # ==================================================================
    # åˆå§‹åŒ–
    # ==================================================================

    def _init_run_context(self, instance_data: Any) -> RunContext:
        """åˆå§‹åŒ–è¿è¡Œä¸Šä¸‹æ–‡"""
        runner = self._ensure_task_runner()
        instance_id = self._get_instance_id(instance_data)

        # åˆå§‹åŒ–è½¨è¿¹è®°å½•å™¨
        trajectory = TrajectoryLogger(
            instance_id,
            self.config.logging.trajectory_dir,
            log_dir=self.config.logging.log_dir,
        )

        # é€šè¿‡ TaskRunner æ„å»º System Prompt
        system_prompt = runner.build_system_prompt(
            instance_data,
            config=self.config,
        )
        trajectory.add_history(role="system", content=system_prompt, message_type="system_prompt")

        # é€šè¿‡ TaskRunner è·å–åˆå§‹è§£
        initial_solution = runner.get_initial_solution(instance_data, self.config)
        if not initial_solution:
            raise ValueError("æ— æ³•è·å–åˆå§‹è§£")

        # åˆå§‹åŒ–å†å²
        self.optimization_history = []

        return RunContext(
            instance_data=instance_data,
            trajectory=trajectory,
            current_solution=initial_solution,
            best_solution=initial_solution,
            best_metric=float("inf"),
            current_artifacts={},
            best_artifacts={},
            optimization_history=self.optimization_history,
        )

    # ==================================================================
    # ä¼˜åŒ–å¾ªç¯
    # ==================================================================

    def _process_optimization_loop(self, ctx: RunContext):
        """æ‰§è¡Œä¼˜åŒ–å¾ªç¯"""
        remaining_iterations = self.config.max_iterations

        self.logger.info(
            f"\n[ä¼˜åŒ–å¾ªç¯å¼€å§‹] è®¡åˆ’è¿­ä»£æ¬¡æ•°: {remaining_iterations}, "
            f"max_iterations: {self.config.max_iterations}"
        )

        for iteration in range(remaining_iterations):
            current_iter_num = iteration + 1

            should_stop = self._process_single_iteration(ctx, current_iter_num)
            if should_stop:
                self.logger.info(f"[ä¼˜åŒ–å¾ªç¯æå‰ç»ˆæ­¢] åœ¨ç¬¬ {current_iter_num} æ¬¡è¿­ä»£ååœæ­¢")
                break

        self.logger.info(f"[ä¼˜åŒ–å¾ªç¯ç»“æŸ] å…±æ‰§è¡Œ {len(ctx.optimization_history)} æ¬¡è¿­ä»£")

    def _process_single_iteration(self, ctx: RunContext, iteration_num: int) -> bool:
        """å¤„ç†å•æ¬¡è¿­ä»£ã€‚è¿”å› True è¡¨ç¤ºåº”è¯¥åœæ­¢å¾ªç¯ã€‚"""
        runner = self._ensure_task_runner()
        iteration_start_time = time.time()
        self.logger.info(
            f"\n{'=' * 60}\n[è¿­ä»£ {iteration_num} å¼€å§‹] æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}\n{'=' * 60}"
        )

        # 1. é€šè¿‡ TaskRunner æ„å»ºä¼˜åŒ– Prompt
        prompt_start = time.time()
        opt_prompt = runner.build_optimization_prompt(
            solution=ctx.current_solution,
            metric=ctx.best_metric,
            artifacts=ctx.current_artifacts,
            config=self.config,
        )
        prompt_elapsed = time.time() - prompt_start
        self.logger.debug(f"[Promptæ„å»º] è€—æ—¶: {prompt_elapsed:.3f}s, Prompté•¿åº¦: {len(opt_prompt)} å­—ç¬¦")

        step_id = ctx.trajectory.start_step(
            "generate_optimization",
            query=opt_prompt,
            code_snapshot=ctx.current_solution,
        )

        # 2. è°ƒç”¨ LLM
        llm_phase_start = time.time()
        system_prompt = runner.build_system_prompt(ctx.instance_data, config=self.config)
        optimization_response = self._call_llm(system_prompt, ctx.trajectory.history, opt_prompt)
        llm_phase_elapsed = time.time() - llm_phase_start

        # 3. é€šè¿‡ TaskRunner æå–æ–°è§£
        extract_start = time.time()
        new_solution = runner.extract_solution(optimization_response, ctx.current_solution)
        extract_elapsed = time.time() - extract_start

        # 4. æ£€æŸ¥è§£æ˜¯å¦å˜åŒ–
        code_changed = new_solution != ctx.current_solution
        self.logger.info(
            f"[è§£å˜æ›´æ£€æŸ¥] å·²å˜æ›´: {code_changed}, "
            f"æ–°è§£é•¿åº¦: {len(new_solution)} å­—ç¬¦"
        )

        if not code_changed:
            self._handle_no_change(ctx, step_id, optimization_response, iteration_num)
            ctx.no_improve_count += 1
            iteration_elapsed = time.time() - iteration_start_time
            self.logger.info(
                f"[è¿­ä»£ {iteration_num} ç»“æŸ] è§£æœªå˜æ›´, è·³è¿‡è¯„ä¼°\n"
                f"  - LLMè°ƒç”¨è€—æ—¶: {llm_phase_elapsed:.2f}s ({llm_phase_elapsed / 60:.1f}åˆ†é’Ÿ)\n"
                f"  - è§£æå–è€—æ—¶: {extract_elapsed:.3f}s\n"
                f"  - è¿­ä»£æ€»è€—æ—¶: {iteration_elapsed:.2f}s ({iteration_elapsed / 60:.1f}åˆ†é’Ÿ)\n"
                f"  - è¿ç»­æœªæ”¹è¿›æ¬¡æ•°: {ctx.no_improve_count}"
            )
            if self.config.early_stop_no_improve and ctx.no_improve_count >= self.config.early_stop_no_improve:
                self.logger.info(f"[æå‰åœæ­¢] è¿ç»­æœªæ”¹è¿›è¾¾åˆ°é˜ˆå€¼ {self.config.early_stop_no_improve}")
                return True
            return False

        # 5. é€šè¿‡ TaskRunner è¯„ä¼°æ–°è§£
        eval_phase_start = time.time()
        try:
            metric, artifacts = runner.evaluate(new_solution, ctx.instance_data, self.config)
            eval_phase_elapsed = time.time() - eval_phase_start

            # æ›´æ–°ä¸Šä¸‹æ–‡çŠ¶æ€
            improved = self._update_run_context_after_eval(
                ctx, new_solution, metric, artifacts, iteration_num
            )

            # è®°å½•æ­¥éª¤
            self._record_iteration_step(
                ctx,
                step_id,
                optimization_response,
                new_solution,
                metric,
                artifacts,
                iteration_num,
                improved,
            )

            if improved:
                ctx.no_improve_count = 0
            else:
                ctx.no_improve_count += 1

            # è¾“å‡ºè¿­ä»£æ€»ç»“
            iteration_elapsed = time.time() - iteration_start_time
            self.logger.info(
                f"\n[è¿­ä»£ {iteration_num} å®Œæˆ] æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}\n"
                f"  â±ï¸  æ—¶é—´åˆ†è§£:\n"
                f"      - LLMè°ƒç”¨è€—æ—¶: {llm_phase_elapsed:.2f}s ({llm_phase_elapsed / 60:.1f}åˆ†é’Ÿ) "
                f"({llm_phase_elapsed / iteration_elapsed * 100:.1f}%)\n"
                f"      - è§£æå–è€—æ—¶: {extract_elapsed:.3f}s\n"
                f"      - è¯„ä¼°è€—æ—¶: {eval_phase_elapsed:.2f}s ({eval_phase_elapsed / 60:.1f}åˆ†é’Ÿ) "
                f"({eval_phase_elapsed / iteration_elapsed * 100:.1f}%)\n"
                f"      - è¿­ä»£æ€»è€—æ—¶: {iteration_elapsed:.2f}s ({iteration_elapsed / 60:.1f}åˆ†é’Ÿ)\n"
                f"  ğŸ“Š metric: {metric}\n"
                f"  âœ… ç»“æœ: {'æ”¹è¿›ï¼Œå·²é‡‡çº³' if improved else 'æœªæ”¹è¿›'}\n"
                f"  ğŸ“ˆ è¿ç»­æœªæ”¹è¿›æ¬¡æ•°: {ctx.no_improve_count}"
            )

            if self.config.early_stop_no_improve and ctx.no_improve_count >= self.config.early_stop_no_improve:
                self.logger.info(f"[æå‰åœæ­¢] è¿ç»­æœªæ”¹è¿›è¾¾åˆ°é˜ˆå€¼ {self.config.early_stop_no_improve}")
                return True

        except Exception as e:
            eval_phase_elapsed = time.time() - eval_phase_start
            iteration_elapsed = time.time() - iteration_start_time
            self.logger.error(
                f"[è¿­ä»£ {iteration_num} å¼‚å¸¸] è¯„ä¼°é˜¶æ®µå‡ºé”™\n"
                f"  - é”™è¯¯ç±»å‹: {type(e).__name__}\n"
                f"  - é”™è¯¯ä¿¡æ¯: {e}\n"
                f"  - LLMè°ƒç”¨è€—æ—¶: {llm_phase_elapsed:.2f}s\n"
                f"  - è¯„ä¼°è€—æ—¶(è‡³å¼‚å¸¸): {eval_phase_elapsed:.2f}s\n"
                f"  - è¿­ä»£æ€»è€—æ—¶: {iteration_elapsed:.2f}s\n"
                f"  - å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}"
            )
            self._handle_evaluation_error(ctx, step_id, optimization_response, iteration_num, str(e))

        return False

    # ==================================================================
    # LLM è°ƒç”¨
    # ==================================================================

    def _call_llm(self, system_prompt: str, history: list[dict[str, Any]], user_prompt: str) -> str:
        """è°ƒç”¨ LLM è·å–å“åº”

        Args:
            system_prompt: ç³»ç»Ÿ promptï¼ˆç”± TaskRunner æ„å»ºï¼‰
            history: å¯¹è¯å†å²
            user_prompt: ç”¨æˆ· promptï¼ˆä¼˜åŒ–æŒ‡ä»¤ï¼‰

        Returns:
            LLM å“åº”æ–‡æœ¬
        """
        messages = self._build_messages(system_prompt, history, user_prompt)

        if self.llm_client:
            llm_start_time = time.time()
            self.logger.info(
                f"[LLMè°ƒç”¨å¼€å§‹] æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}, æ¨¡å‹: {self.config.model.name}"
            )
            try:
                response = self.llm_client.call_llm(
                    messages,
                    temperature=self.config.model.temperature,
                    max_tokens=self.config.model.max_output_tokens,
                    usage_context="perfagent.optimize",
                )
                llm_elapsed = time.time() - llm_start_time
                self.logger.info(
                    f"[LLMè°ƒç”¨å®Œæˆ] è€—æ—¶: {llm_elapsed:.2f}s ({llm_elapsed / 60:.1f}åˆ†é’Ÿ), "
                    f"å“åº”é•¿åº¦: {len(response)} å­—ç¬¦"
                )
                return response
            except Exception as e:
                llm_elapsed = time.time() - llm_start_time
                self.logger.error(
                    f"[LLMè°ƒç”¨å¤±è´¥] è€—æ—¶: {llm_elapsed:.2f}s, é”™è¯¯ç±»å‹: {type(e).__name__}, "
                    f"é”™è¯¯ä¿¡æ¯: {e}\n{traceback.format_exc()}"
                )
                raise
        self.logger.warning("[LLMæœªé…ç½®] LLM å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æœ¬æ¬¡ä¼˜åŒ–")
        return "LLM æœªé…ç½®æˆ–ä¸å¯ç”¨ï¼Œè·³è¿‡æœ¬æ¬¡ä¼˜åŒ–å»ºè®®ã€‚è¯·æ£€æŸ¥ API é…ç½®ã€‚"

    # ==================================================================
    # ä¸Šä¸‹æ–‡æ›´æ–° & æ­¥éª¤è®°å½•
    # ==================================================================

    def _update_run_context_after_eval(
        self, ctx: RunContext, new_solution: str, metric: float, artifacts: dict[str, Any], iteration: int
    ) -> bool:
        """æ›´æ–°ä¸Šä¸‹æ–‡å¹¶åˆ¤æ–­æ˜¯å¦æ”¹è¿›

        ä½¿ç”¨é€šç”¨çš„ metric æ¯”è¾ƒï¼ˆè¶Šä½è¶Šå¥½ï¼‰ã€‚TaskRunner çš„ evaluate() æ–¹æ³•
        è´Ÿè´£ç¡®ä¿ metric è¯­ä¹‰ä¸€è‡´ï¼ˆå¦‚æµ‹è¯•æœªé€šè¿‡æ—¶è¿”å› infï¼‰ã€‚
        """
        improved = metric < ctx.best_metric

        # å¦‚æœæœ€å¤§è¿­ä»£æ¬¡æ•°ä¸º 1ï¼Œå¼ºåˆ¶è§†ä¸ºæ”¹è¿›ï¼ˆå³æ€»æ˜¯ä¿å­˜ç”Ÿæˆä»£ç ï¼‰
        if self.config.max_iterations == 1 and not improved:
            improved = True
            self.logger.info("å•æ¬¡è¿­ä»£æ¨¡å¼ï¼šå¼ºåˆ¶é‡‡çº³ç”Ÿæˆè§£ä½œä¸ºæœ€ä½³ç»“æœ")

        # è®°å½•å†å²
        ctx.optimization_history.append(
            {
                "iteration": iteration,
                "metric_before": ctx.best_metric,
                "metric_after": metric,
                "improvement": ctx.best_metric - metric,
                "success": improved,
            }
        )

        if improved:
            ctx.best_metric = metric
            ctx.best_solution = new_solution
            ctx.best_artifacts = artifacts
            self.logger.info(f"é‡‡ç”¨æ›´ä¼˜è§£: metric {ctx.best_metric}")
        else:
            self.logger.info(f"æœªæ”¹è¿›: metric {metric} vs best {ctx.best_metric}")

        # å†³å®šæ˜¯å¦é‡‡ç”¨è§£
        if self.config.adopt_only_if_improved:
            if improved:
                ctx.current_solution = new_solution
            else:
                ctx.current_solution = ctx.best_solution
        else:
            ctx.current_solution = new_solution

        ctx.current_artifacts = artifacts
        return improved

    def _record_iteration_step(
        self,
        ctx: RunContext,
        step_id: str,
        response: str,
        new_solution: str,
        metric: float,
        artifacts: dict[str, Any],
        iteration: int,
        improved: bool,
    ):
        """è®°å½•è¿­ä»£æ­¥éª¤åˆ°è½¨è¿¹"""
        adopted = improved if self.config.adopt_only_if_improved else True

        summary_text = self._build_summary_text(
            iteration=iteration,
            code_changed=adopted,
            solution=ctx.current_solution,
            metric=metric,
            artifacts=artifacts,
        )

        ctx.trajectory.end_step(
            step_id,
            response=response,
            thought=("åº”ç”¨ä¼˜åŒ–å¹¶å®Œæˆè¯„ä¼°" if adopted else "è¯„ä¼°æœªæ”¹è¿›ï¼Œæœªé‡‡ç”¨ä¼˜åŒ–"),
            code_changed=adopted,
            performance_metrics={"metric": metric, **(artifacts or {})},
            code_snapshot=ctx.current_solution,
            summary=summary_text,
        )

    # ==================================================================
    # é”™è¯¯å¤„ç†
    # ==================================================================

    def _handle_no_change(self, ctx: RunContext, step_id: str, response: str, iteration: int):
        """å¤„ç†è§£æœªå˜æ›´çš„æƒ…å†µ"""
        summary = self._build_summary_text(
            iteration=iteration,
            code_changed=False,
            solution=ctx.current_solution,
            metric=ctx.best_metric,
            artifacts=ctx.current_artifacts,
        )
        ctx.trajectory.end_step(
            step_id,
            response=response,
            thought="è§£æå–åæœªå˜åŒ–ï¼Œè·³è¿‡",
            code_changed=False,
            code_snapshot=ctx.current_solution,
            summary=summary,
        )
        self.logger.warning("è§£æœªå‘ç”Ÿå˜åŒ–ï¼Œè·³è¿‡æ­¤æ¬¡è¿­ä»£")

    def _handle_evaluation_error(
        self, ctx: RunContext, step_id: str, response: str, iteration: int, error_msg: str
    ):
        """å¤„ç†è¯„ä¼°å¼‚å¸¸"""
        summary = self._build_summary_text(
            iteration=iteration,
            code_changed=True,
            solution=ctx.current_solution,
            error_message=f"è¯„ä¼°å¤±è´¥: {error_msg}",
        )
        ctx.trajectory.end_step(
            step_id,
            response=response,
            thought="è¯„ä¼°é˜¶æ®µå‘ç”Ÿå¼‚å¸¸",
            code_changed=True,
            performance_metrics=None,
            error=f"è¯„ä¼°å¤±è´¥: {error_msg}",
            code_snapshot=ctx.current_solution,
            summary=summary,
        )

    # ==================================================================
    # ç»“æœæ±‡æ€»
    # ==================================================================

    def _finalize_run(self, ctx: RunContext) -> dict[str, Any]:
        """å®Œæˆè¿è¡Œå¹¶ç”Ÿæˆæœ€ç»ˆç»“æœ"""
        finalize_start = time.time()
        self.logger.info(f"\n[ç»“æœæ±‡æ€»å¼€å§‹] æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")

        instance_id = self._get_instance_id(ctx.instance_data)
        best_metric = ctx.best_metric
        executed_iterations = len(ctx.optimization_history)

        # åªè¦æœ‰æœ‰æ•ˆ metric å°±ç®—æˆåŠŸ
        success = bool(best_metric < float("inf"))

        # æ„å»ºæœ€ç»ˆ artifactsï¼ˆç¡®ä¿åŒ…å« problem_descriptionï¼‰
        artifacts = dict(ctx.best_artifacts)
        artifacts.setdefault("problem_description", "")
        artifacts["optimization_history"] = ctx.optimization_history

        # è®°å½•æœ€ç»ˆè½¨è¿¹
        trajectory_file = ctx.trajectory.finalize(
            success=success,
            final_performance={"metric": best_metric},
            final_submission_code=ctx.best_solution,
        )

        # ä¸»ç»“æœï¼ˆAgentResult æ ¼å¼ï¼‰
        final_result: dict[str, Any] = {
            "instance_id": instance_id,
            "success": success,
            "solution": ctx.best_solution,
            "metric": best_metric,
            "artifacts": artifacts,
            "total_iterations": executed_iterations,
            "trajectory_file": trajectory_file,
            "error": None,
        }

        # ç»Ÿè®¡ä¼˜åŒ–å†å²
        successful_iterations = sum(1 for h in ctx.optimization_history if h.get("success", False))

        finalize_elapsed = time.time() - finalize_start
        self.logger.info(
            f"\n[ä¼˜åŒ–ç»“æœæ€»ç»“]\n"
            f"  ğŸ“‹ åŸºæœ¬ä¿¡æ¯:\n"
            f"      - å®ä¾‹ID: {instance_id}\n"
            f"      - æ‰§è¡Œè¿­ä»£æ•°: {executed_iterations}\n"
            f"      - æˆåŠŸæ”¹è¿›è¿­ä»£æ•°: {successful_iterations}\n"
            f"\n"
            f"  ğŸ“ˆ æ€§èƒ½å˜åŒ–:\n"
            f"      - æœ€ç»ˆ metric: {best_metric}\n"
            f"      - ä¼˜åŒ–æˆåŠŸ: {'âœ… æ˜¯' if success else 'âŒ å¦'}\n"
            f"\n"
            f"  ğŸ“ è½¨è¿¹æ–‡ä»¶: {trajectory_file}\n"
            f"  â±ï¸  ç»“æœæ±‡æ€»è€—æ—¶: {finalize_elapsed:.3f}s"
        )

        return final_result

    # ==================================================================
    # é€šç”¨è¾…åŠ©æ–¹æ³•
    # ==================================================================

    @staticmethod
    def _clean_performance_value(val: Any) -> float:
        """æ¸…ç†æ€§èƒ½æŒ‡æ ‡å€¼ï¼Œè½¬æ¢ä¸º floatï¼Œå¤„ç† inf/nan"""
        if isinstance(val, (int, float)):
            return float(val)

        # å°è¯•å¤„ç† numpy ç±»å‹æˆ– callable
        try:
            item_fn = getattr(val, "item", None)
            if callable(item_fn):
                val = item_fn()
        except Exception:
            pass

        if isinstance(val, str):
            s = val.strip().lower()
            if s in ("inf", "+inf", "infinity", "+infinity"):
                return float("inf")
            elif s in ("-inf", "-infinity"):
                return float("-inf")
            elif s == "nan":
                return float("nan")
            else:
                try:
                    return float(val)
                except Exception:
                    return float("inf")
        return float(val) if isinstance(val, (int, float)) else float("inf")

    @staticmethod
    def _format_artifacts_md(artifacts: dict[str, Any]) -> str:
        """å°† artifacts å­—å…¸æ ¼å¼åŒ–ä¸º Markdown æ–‡æœ¬"""
        if not artifacts:
            return "- No artifacts available."
        lines: list[str] = []
        for k, v in artifacts.items():
            if isinstance(v, str) and "\n" in v:
                indented = "\n  ".join(v.split("\n"))
                lines.append(f"- {k}: {indented}")
            else:
                lines.append(f"- {k}: {v}")
        return "\n".join(lines)

    def _build_summary_text(
        self,
        iteration: int,
        code_changed: bool,
        solution: str | None = None,
        metric: float | None = None,
        artifacts: dict[str, Any] | None = None,
        error_message: str | None = None,
    ) -> str:
        """æ„å»ºä¸€æ­¥è¿­ä»£çš„ Markdown æ‘˜è¦æ–‡æœ¬"""
        parts: list[str] = [
            "## Program Update",
            f"- Iteration: {iteration}",
            f"- Code changed: {code_changed}",
        ]

        if metric is not None:
            parts.append(f"- Metric: {metric}")

        if error_message:
            parts.append(f"- Error: {error_message}")

        parts.append("")
        parts.append("## Current Solution")
        parts.append(solution or "")

        if artifacts:
            parts.append("")
            parts.append("## Current Artifacts")
            parts.append(self._format_artifacts_md(artifacts))

        return "\n".join(parts)

    def _build_messages(
        self, system_prompt: str, history: list[dict[str, Any]], user_prompt: str, limit: int = 200
    ) -> list[dict[str, str]]:
        """æ„å»º LLM æ¶ˆæ¯åˆ—è¡¨"""
        use_all = bool(getattr(self.config.prompts, "include_all_history", False))
        if use_all:
            msgs: list[dict[str, str]] = []
            tail = history[-limit:] if len(history) > limit else history
            for h in tail:
                role = h.get("role")
                content = h.get("content", "")
                if role in ("system", "user", "assistant") and content:
                    msgs.append({"role": role, "content": content})
            return msgs
        return [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
