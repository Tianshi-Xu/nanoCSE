"""
PerfAgent æ ¸å¿ƒç±»

å®ç°ä»£ç æ€§èƒ½ä¼˜åŒ–çš„ä¸»è¦é€»è¾‘ï¼ŒåŒ…æ‹¬è¿­ä»£ä¼˜åŒ–ã€diff åº”ç”¨ã€æ€§èƒ½è¯„ä¼°ç­‰åŠŸèƒ½ã€‚
"""

import json
import logging
import re
import time
import traceback
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any

from .config import PerfAgentConfig
from .diff_applier import DiffApplier
from .effibench.benchmark import run_performance_benchmark
from .effibench.utils import EFFIBENCH_REGISTRY
from .llm_client import LLMClient
from .trajectory import TrajectoryLogger
from .utils.log import get_se_logger


@dataclass
class EffiBenchXInstance:
    id: str
    title: str
    title_slug: str
    description: str
    description_md: str
    source: str
    url: str
    type: str
    starter_code: str | None = None
    solutions: dict[str, dict[str, str]] = field(default_factory=dict)
    language: str | None = None
    generated_tests: list[dict[str, Any]] = field(default_factory=list)
    evaluator: str | None = None
    test_runners: dict[str, str] = field(default_factory=dict)
    # ä»»åŠ¡åï¼ˆæ¥æºäºå®ä¾‹æ–‡ä»¶åï¼Œä¸å«æ‰©å±•åï¼‰
    task_name: str | None = None

    @staticmethod
    def from_dict(data: dict[str, Any]) -> "EffiBenchXInstance":
        # Robustly parse generated_tests when it can be a list or a JSON string
        gt_raw = data.get("generated_tests", [])
        if isinstance(gt_raw, str):
            try:
                gt_parsed = json.loads(gt_raw)
            except Exception:
                gt_parsed = []
        elif isinstance(gt_raw, list):
            gt_parsed = gt_raw
        else:
            gt_parsed = []

        # Robustly parse test_runners when it can be a dict or a JSON string
        tr_raw = data.get("test_runners", {})
        if isinstance(tr_raw, str):
            try:
                tr_parsed = json.loads(tr_raw)
            except Exception:
                tr_parsed = {}
        elif isinstance(tr_raw, dict):
            tr_parsed = tr_raw
        else:
            tr_parsed = {}

        return EffiBenchXInstance(
            id=str(data.get("id", "unknown")),
            title=data.get("title", ""),
            title_slug=data.get("title_slug", ""),
            description=data.get("description", ""),
            description_md=data.get("description_md", ""),
            source=data.get("source", ""),
            url=data.get("url", ""),
            type=data.get("type", ""),
            starter_code=data.get("starter_code"),
            solutions=data.get("solutions", {}),
            language=data.get("language"),
            generated_tests=gt_parsed,
            evaluator=data.get("evaluator"),
            test_runners=tr_parsed,
        )

    def to_dict(self) -> dict[str, Any]:
        return {
            "id": self.id,
            "title": self.title,
            "title_slug": self.title_slug,
            "description": self.description,
            "description_md": self.description_md,
            "source": self.source,
            "url": self.url,
            "type": self.type,
            "starter_code": self.starter_code,
            "solutions": self.solutions,
            "language": self.language,
            "generated_tests": self.generated_tests,
            "evaluator": self.evaluator,
            "test_runners": self.test_runners,
            "task_name": self.task_name,
        }


@dataclass
class RunContext:
    """ä¿å­˜å•æ¬¡è¿è¡Œçš„ä¸Šä¸‹æ–‡çŠ¶æ€"""

    instance: EffiBenchXInstance
    trajectory: TrajectoryLogger
    language: str
    optimization_target: str
    initial_code: str
    current_code: str
    best_code: str
    best_performance: float
    best_pass_rate: float
    current_benchmark_results: dict[str, Any]
    best_benchmark_results: dict[str, Any]
    optimization_history: list[dict[str, Any]]
    iter_offset: int
    no_improve_count: int = 0
    test_cases: list[dict[str, Any]] = field(default_factory=list)
    initial_performance_value: float = float("inf")


class PerfAgent:
    """æ€§èƒ½ä¼˜åŒ– Agent"""

    def __init__(self, config: PerfAgentConfig):
        self.config = config

        # ç®€åŒ–é€»è¾‘ï¼šå‡­æ®å­˜åœ¨å³åˆå§‹åŒ– LLMClientï¼Œæ— éœ€ use_llm æ ‡å¿—
        self.llm_client = None
        if self.config.model.api_base and self.config.model.api_key:
            client_cfg = {
                "name": self.config.model.name,
                "api_base": self.config.model.api_base,
                "api_key": self.config.model.api_key,
                "max_output_tokens": self.config.model.max_output_tokens,
                "request_timeout": self.config.model.request_timeout,
                "max_retries": self.config.model.max_retries,
                "retry_delay": self.config.model.retry_delay,
                "retry_backoff_factor": getattr(self.config.model, "retry_backoff_factor", 2.0),
                "retry_jitter": getattr(self.config.model, "retry_jitter", 0.5),
                "log_inputs_outputs": self.config.model.log_inputs_outputs,
                "log_sanitize": self.config.model.log_sanitize,
            }
            # å°† LLM I/O ç‹¬ç«‹å†™å…¥ log_dir/llm_io.log
            io_log_file = Path(self.config.logging.log_dir) / "llm_io.log"
            self.llm_client = LLMClient(
                client_cfg,
                io_log_path=io_log_file,
                log_inputs_outputs=self.config.model.log_inputs_outputs,
                log_sanitize=self.config.model.log_sanitize,
                request_timeout=self.config.model.request_timeout,
            )

        self.diff_applier = DiffApplier()

        # è®¾ç½®æ—¥å¿—ï¼šç»Ÿä¸€ç»‘å®šåˆ°å•ä¸€æ–‡ä»¶
        # ä½¿ç”¨åŒ…å«æ—¥å¿—ç›®å½•åçš„å”¯ä¸€ logger åç§°ï¼Œé¿å…å¹¶å‘å®ä¾‹å¤ç”¨åŒåå¯¼è‡´ä¸²å†™
        agent_logger_name = f"perfagent.agent.{Path(self.config.logging.log_dir).name}"
        get_se_logger(
            agent_logger_name,
            Path(self.config.logging.log_dir) / "perfagent.log",
            emoji="ğŸ”§",
            level=getattr(logging, self.config.logging.log_level.upper()),
            also_stream=False,
        )
        self.logger = logging.getLogger(agent_logger_name)

        # ä¼˜åŒ–å†å²
        self.optimization_history: list[dict[str, Any]] = []

        # åˆå§‹ä»£ç æ¥æºï¼š"default" | "text" | "dir"
        self._initial_code_source: str = "default"

    def _normalize_language(self, lang: str | None) -> str:
        # æ ‡å‡†åŒ–è¯­è¨€åç§°
        if not lang:
            return "python3"
        l = lang.lower()
        if l in ("python", "py", "python3"):
            return "python3"
        if l in ("cpp", "c++", "cxx"):
            return "cpp"
        if l in ("javascript", "js"):
            return "javascript"
        if l in ("java",):
            return "java"
        return l

    def _get_default_placeholder(self, language: str | None = None) -> str:
        """è·å–é»˜è®¤å ä½ç¬¦ä»£ç ï¼ˆæ ¹æ®è¯­è¨€ï¼‰"""
        lang = self._normalize_language(language or self.config.language_cfg.language)
        placeholder_map = {
            "python3": "# Start your code here\n",
            "cpp": "// Start your code here\n",
            "java": "// Start your code here\n",
            "javascript": "// Start your code here\n",
            "golang": "// Start your code here\n",
        }
        return placeholder_map.get(lang, "# Start your code here\n")

    def _extract_initial_code(
        self, instance: EffiBenchXInstance, language: str | None = None, optimization_target: str | None = None
    ) -> str:
        """ä»é…ç½®/æ–‡ä»¶ç³»ç»Ÿæ³¨å…¥æˆ–ç”Ÿæˆåˆå§‹ä»£ç ã€‚

        ä¼˜å…ˆçº§ï¼š
        1) é…ç½® overrides.initial_code_textï¼ˆç›´æ¥æ–‡æœ¬ï¼‰
        2) é…ç½® overrides.initial_code_dirï¼ˆæŒ‰å®ä¾‹ååŒ¹é…æ–‡ä»¶ï¼‰
        3) é»˜è®¤å ä½ç¬¦ä»£ç ï¼ˆæ ¹æ®è¯­è¨€ï¼‰
        """
        try:
            # é»˜è®¤æ¥æº
            self._initial_code_source = "default"
            # 1) ç›´æ¥æ–‡æœ¬è¦†ç›–
            override_text = getattr(getattr(self.config, "overrides", None), "initial_code_text", None)
            if isinstance(override_text, str) and override_text.strip():
                self._initial_code_source = "text"
                return override_text if override_text.endswith("\n") else override_text + "\n"

            # 2) ç›®å½•è¦†ç›–ï¼ˆæŒ‰å®ä¾‹ååŒ¹é…æ–‡ä»¶ï¼‰
            code_dir = getattr(getattr(self.config, "overrides", None), "initial_code_dir", None)
            task_name = getattr(instance, "task_name", None) or getattr(instance, "id", None)
            if code_dir and task_name:
                lang = self._normalize_language(language or self.config.language_cfg.language)
                # è¯­è¨€æ‰©å±•æ˜ å°„
                ext_map = {
                    "python3": [".py"],
                    "cpp": [".cpp", ".cc", ".cxx"],
                    "java": [".java"],
                    "javascript": [".js", ".mjs"],
                    "golang": [".go"],
                }
                candidates: list[Path] = []
                for ext in ext_map.get(lang, []):
                    candidates.append(Path(code_dir) / f"{task_name}{ext}")
                # é€€åŒ–ï¼šä»»æ„åŒ¹é…åŒåæ–‡ä»¶ï¼ˆä¸åŒºåˆ†æ‰©å±•åï¼‰
                try:
                    for fp in Path(code_dir).iterdir():
                        if fp.is_file() and fp.stem == task_name and fp not in candidates:
                            candidates.append(fp)
                except Exception:
                    pass

                for fp in candidates:
                    try:
                        if fp.exists():
                            code = fp.read_text(encoding="utf-8")
                            if isinstance(code, str) and code.strip():
                                self.logger.info(f"ä½¿ç”¨è¦†ç›–åˆå§‹ä»£ç : {fp}")
                                self._initial_code_source = "dir"
                                return code if code.endswith("\n") else code + "\n"
                    except Exception as e:
                        self.logger.warning(f"è¯»å–åˆå§‹ä»£ç æ–‡ä»¶å¤±è´¥ {fp}: {e}")
        except Exception as e:
            # è¦†ç›–æµç¨‹å¤±è´¥åˆ™å›é€€åˆ°å ä½ç¬¦
            self.logger.warning(f"åˆå§‹ä»£ç è¦†ç›–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å ä½ç¬¦: {e}")

        # 3) é»˜è®¤å ä½ç¬¦ï¼ˆä¿æŒç°æœ‰æµ‹è¯•å…¼å®¹ï¼‰
        return self._get_default_placeholder(language)

    def _resolve_starter_code(self, instance: EffiBenchXInstance, language: str) -> str | None:
        sc = getattr(instance, "starter_code", None)
        if isinstance(sc, dict):
            try:
                return sc.get(language)
            except Exception:
                return None
        if isinstance(sc, str):
            return sc
        return None

    def _resolve_test_runner(self, instance: EffiBenchXInstance, language: str) -> str | None:
        trs = getattr(instance, "test_runners", None)
        if isinstance(trs, dict):
            try:
                lang_norm = self._normalize_language(language)
                val = trs.get(lang_norm)
                if isinstance(val, str) and val.strip():
                    return val
            except Exception:
                return None
        if isinstance(trs, str):
            try:
                parsed = json.loads(trs)
                if isinstance(parsed, dict):
                    val = parsed.get(self._normalize_language(language))
                    if isinstance(val, str) and val.strip():
                        return val
            except Exception:
                pass
        return None

    def _prepare_test_cases(self, instance: EffiBenchXInstance) -> list[dict[str, Any]]:
        """å‡†å¤‡æµ‹è¯•ç”¨ä¾‹ï¼ˆå®ä¾‹ä»…ä¸º dataclassï¼‰"""
        return instance.generated_tests or []

    def _detect_language(self, instance: EffiBenchXInstance) -> str:
        """æ£€æµ‹ç¼–ç¨‹è¯­è¨€ï¼ˆä»…ä¿ç•™ä»¥å…¼å®¹è°ƒç”¨è·¯å¾„ï¼Œä½†ä¸ä½¿ç”¨ï¼‰"""
        return self._normalize_language(self.config.language_cfg.language)

    def _create_empty_performance_metrics(self) -> dict[str, Any]:
        """åˆ›å»ºä¸€ä¸ªç©ºçš„æ€§èƒ½åˆ†ææŒ‡æ ‡ç»“æ„"""
        return {
            "original_n": 0,
            "n": 0,
            "runtime": float("inf"),
            "memory": float("inf"),
            "integral": float("inf"),
            "pass_rate": 0.0,
            "passed": False,
            "analysis": {
                "runtime": self._create_empty_metric_analysis(),
                "memory": self._create_empty_metric_analysis(),
                "integral": self._create_empty_metric_analysis(),
            },
        }

    def _create_empty_metric_analysis(self) -> dict[str, Any]:
        """åˆ›å»ºä¸€ä¸ªç©ºçš„å•é¡¹æŒ‡æ ‡åˆ†æç»“æ„"""
        return {
            "original_n": 0,
            "n": 0,
            "mean": float("inf"),
            "std": float("inf"),
            "min": float("inf"),
            "max": float("inf"),
            "max_diff": float("inf"),
            "95%_CI": (float("inf"), float("inf")),
            "trimmed_mean": float("inf"),
        }

    def _create_default_performance_result(self, consistent: bool = True) -> dict[str, Any]:
        """åˆ›å»ºé»˜è®¤çš„æ€§èƒ½è¯„ä¼°ç»“æœç»“æ„"""
        return {
            "performance_analysis": self._create_empty_performance_metrics(),
            "first_run_details": [],
            "failed_test_details": [],
            "failed_submission_exit_codes": [],
            "pass_rates": [],
            "pass_rate_consistent": consistent,
        }

    def _evaluate_performance(
        self, language: str, code: str, test_cases: list[dict], instance: EffiBenchXInstance
    ) -> dict[str, Any]:
        """è¯„ä¼°ä»£ç æ€§èƒ½ï¼Œä¿æŒå‚æ•°å…¼å®¹"""
        eval_start_time = time.time()
        self.logger.info(
            f"[æ€§èƒ½è¯„ä¼°å¼€å§‹] æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}, "
            f"æµ‹è¯•ç”¨ä¾‹æ•°: {len(test_cases)}, ä»£ç é•¿åº¦: {len(code)} å­—ç¬¦"
        )

        # å¦‚æœä»£ç ä¸å ä½ç¬¦ä»£ç ç›¸åŒï¼Œè¿”å›é»˜è®¤å¤±è´¥ç»“æ„
        if code == self._get_default_placeholder(language):
            self.logger.info("[æ€§èƒ½è¯„ä¼°è·³è¿‡] ä»£ç ä¸ºå ä½ç¬¦ï¼Œè¿”å›é»˜è®¤å¤±è´¥ç»“æ„")
            return self._create_default_performance_result(consistent=True)

        # è‹¥ evaluator æˆ–æµ‹è¯•ç”¨ä¾‹ç¼ºå¤±/æ ¼å¼ä¸åˆæ³•ï¼Œç›´æ¥è¿”å›é»˜è®¤ç»“æ„ä»¥é¿å…é•¿æ—¶é—´çš„åç«¯è°ƒç”¨
        evaluator = getattr(instance, "evaluator", None)
        tc_valid = bool(test_cases) and isinstance(test_cases, list) and isinstance(test_cases[0], dict)
        if not evaluator or not tc_valid:
            self.logger.warning(
                f"[æ€§èƒ½è¯„ä¼°è·³è¿‡] ç¼ºå°‘å¿…è¦ç»„ä»¶ - evaluator: {bool(evaluator)}, test_casesæœ‰æ•ˆ: {tc_valid}"
            )
            return self._create_default_performance_result(consistent=True)
        test_runner = self._resolve_test_runner(instance, language)

        # çº§è”è¯„ä¼°ï¼šå…ˆç”¨ benchmark è¿›è¡Œä¸€æ¬¡è¿è¡Œï¼ˆnum_runs=1ï¼‰ï¼Œè‹¥æœªå…¨éƒ¨é€šè¿‡åˆ™ç›´æ¥è¿”å›
        single_run_start = time.time()
        self.logger.info("[å•æ¬¡é¢„è¿è¡Œå¼€å§‹] éªŒè¯ä»£ç æ­£ç¡®æ€§...")
        try:
            single_run_summary = run_performance_benchmark(
                lang=language,
                solution=code,
                test_cases=test_cases,
                evaluator=evaluator,
                test_runner=test_runner,
                num_runs=1,
                time_limit=self.config.runtime.time_limit,
                memory_limit=self.config.runtime.memory_limit,
                trim_ratio=self.config.runtime.trim_ratio,
                max_workers=self.config.runtime.max_workers,
            )
            single_run_elapsed = time.time() - single_run_start
            pass_rate = single_run_summary.get("performance_analysis", {}).get("pass_rate", 0)
            self.logger.info(f"[å•æ¬¡é¢„è¿è¡Œå®Œæˆ] è€—æ—¶: {single_run_elapsed:.2f}s, é€šè¿‡ç‡: {pass_rate:.2%}")
        except Exception as e:
            single_run_elapsed = time.time() - single_run_start
            self.logger.warning(
                f"[å•æ¬¡é¢„è¿è¡Œå¤±è´¥] è€—æ—¶: {single_run_elapsed:.2f}s, é”™è¯¯ç±»å‹: {type(e).__name__}, "
                f"é”™è¯¯ä¿¡æ¯: {e}\n{traceback.format_exc()}"
            )
            return self._create_default_performance_result(consistent=True)

        # è®¡ç®—å•æ¬¡è¿è¡Œé€šè¿‡ç‡ï¼ˆä¼˜å…ˆä½¿ç”¨è¿”å›çš„ pass_ratesï¼‰
        passed = single_run_summary.get("performance_analysis").get("passed", False)

        # è‹¥æœªå…¨éƒ¨é€šè¿‡ï¼Œç›´æ¥è¿”å›å•æ¬¡è¿è¡Œçš„ç»“æœï¼ˆä¸è¿›è¡Œå¤šæ¬¡æ€§èƒ½è¯„ä¼°ï¼‰
        if not passed:
            total_elapsed = time.time() - eval_start_time
            failed_count = len(single_run_summary.get("failed_test_details", []))
            self.logger.info(
                f"[æ€§èƒ½è¯„ä¼°æå‰ç»“æŸ] ä»£ç æœªå…¨éƒ¨é€šè¿‡æµ‹è¯•ï¼Œå¤±è´¥ç”¨ä¾‹æ•°: {failed_count}, æ€»è€—æ—¶: {total_elapsed:.2f}s"
            )
            return single_run_summary

        # è‹¥é‡å¤è¿è¡Œæ¬¡æ•°ä¸º 1ï¼Œç›´æ¥è¿”å›å•æ¬¡è¿è¡Œçš„ç»“æœï¼Œæ— éœ€è¿›è¡Œçº§è”è¯„ä¼°
        if self.config.runtime.num_runs == 1:
            total_elapsed = time.time() - eval_start_time
            perf_analysis = single_run_summary.get("performance_analysis", {})
            self.logger.info(
                f"[æ€§èƒ½è¯„ä¼°å®Œæˆ] å•æ¬¡è¿è¡Œæ¨¡å¼, æ€»è€—æ—¶: {total_elapsed:.2f}s, "
                f"runtime: {perf_analysis.get('runtime', 'N/A')}s, "
                f"memory: {perf_analysis.get('memory', 'N/A')}MB"
            )
            return single_run_summary

        # æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹é€šè¿‡ï¼Œè¿›è¡Œæ­£å¼çš„å¤šæ¬¡æ€§èƒ½è¯„ä¼°
        multi_run_start = time.time()
        self.logger.info(
            f"[å¤šæ¬¡è¯„ä¼°å¼€å§‹] è¿è¡Œæ¬¡æ•°: {self.config.runtime.num_runs}, "
            f"time_limit: {self.config.runtime.time_limit}s, memory_limit: {self.config.runtime.memory_limit}MB"
        )
        try:
            result = run_performance_benchmark(
                lang=language,
                solution=code,
                test_cases=test_cases,
                evaluator=evaluator,
                test_runner=test_runner,
                num_runs=self.config.runtime.num_runs,
                time_limit=self.config.runtime.time_limit,
                memory_limit=self.config.runtime.memory_limit,
                trim_ratio=self.config.runtime.trim_ratio,
                max_workers=self.config.runtime.max_workers,
            )
            multi_run_elapsed = time.time() - multi_run_start
            total_elapsed = time.time() - eval_start_time
            perf_analysis = result.get("performance_analysis", {})
            self.logger.info(
                f"[å¤šæ¬¡è¯„ä¼°å®Œæˆ] å¤šæ¬¡è¿è¡Œè€—æ—¶: {multi_run_elapsed:.2f}s, æ€»è¯„ä¼°è€—æ—¶: {total_elapsed:.2f}s\n"
                f"  - runtime: {perf_analysis.get('runtime', 'N/A')}s (trimmed_mean)\n"
                f"  - memory: {perf_analysis.get('memory', 'N/A')}MB (trimmed_mean)\n"
                f"  - integral: {perf_analysis.get('integral', 'N/A')}MB*s\n"
                f"  - pass_rate: {perf_analysis.get('pass_rate', 0):.2%}"
            )
            return result
        except Exception as e:
            multi_run_elapsed = time.time() - multi_run_start
            total_elapsed = time.time() - eval_start_time
            self.logger.error(
                f"[å¤šæ¬¡è¯„ä¼°å¤±è´¥] å¤šæ¬¡è¿è¡Œè€—æ—¶: {multi_run_elapsed:.2f}s, æ€»è€—æ—¶: {total_elapsed:.2f}s, "
                f"é”™è¯¯ç±»å‹: {type(e).__name__}, é”™è¯¯ä¿¡æ¯: {e}\n{traceback.format_exc()}"
            )
            return self._create_default_performance_result(consistent=False)

    def _extract_pass_rate(self, results: dict[str, Any]) -> float:
        """ä»è¯„ä¼°ç»“æœä¸­æå–é€šè¿‡ç‡"""
        # 1. å°è¯•ç›´æ¥è·å– pass_rate å­—æ®µ
        pass_rate = results.get("performance_analysis", {}).get("pass_rate")
        try:
            if pass_rate is not None and isinstance(pass_rate, (int, float)):
                return float(pass_rate)
        except Exception:
            pass

        # 2. å°è¯•ä» pass_rates åˆ—è¡¨è·å–ï¼ˆå–æœ€å°å€¼ï¼Œä¿å®ˆç­–ç•¥ï¼‰
        pr_list = results.get("pass_rates")
        try:
            if isinstance(pr_list, list) and pr_list:
                return float(min(float(p) for p in pr_list))
        except Exception:
            pass

        # 3. å°è¯•ä» first_run_details è®¡ç®—
        try:
            fr = results.get("first_run_details") or []
            total = len(fr)
            passed = sum(1 for tc in fr if tc.get("passed", False))
            return (passed / total) if total > 0 else 0.0
        except Exception:
            return 0.0

    def _clean_performance_value(self, val: Any) -> float:
        """æ¸…ç†æ€§èƒ½æŒ‡æ ‡å€¼ï¼Œè½¬æ¢ä¸º floatï¼Œå¤„ç† inf/nan"""
        if isinstance(val, (int, float)):
            return float(val)

        # å°è¯•å¤„ç† numpy ç±»å‹æˆ– callable
        try:
            item_fn = getattr(val, "item", None)
            if callable(item_fn):
                val = item_fn()
        except Exception:
            pass

        if isinstance(val, str):
            s = val.strip().lower()
            if s in ("inf", "+inf", "infinity", "+infinity"):
                return float("inf")
            elif s in ("-inf", "-infinity"):
                return float("-inf")
            elif s == "nan":
                return float("nan")
            else:
                try:
                    return float(val)
                except Exception:
                    return float("inf")
        return float(val) if isinstance(val, (int, float)) else float("inf")

    def run(self, instance: EffiBenchXInstance) -> dict[str, Any]:
        """è¿è¡Œæ€§èƒ½ä¼˜åŒ–æµç¨‹ï¼ˆä»…ä½¿ç”¨é…ç½®è¯­è¨€ï¼Œå®ä¾‹ä¸º dataclassï¼‰"""
        run_start_time = time.time()
        instance_id = getattr(instance, "task_name", None) or getattr(instance, "id", "unknown")

        self.logger.info(
            f"\n{'#' * 70}\n"
            f"# [PerfAgent è¿è¡Œå¼€å§‹]\n"
            f"# æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            f"# å®ä¾‹: {instance_id}\n"
            f"# æœ€å¤§è¿­ä»£æ¬¡æ•°: {self.config.max_iterations}\n"
            f"# ä¼˜åŒ–ç›®æ ‡: {self.config.optimization.target}\n"
            f"# æ¨¡å‹: {self.config.model.name}\n"
            f"{'#' * 70}"
        )

        try:
            # 1. åˆå§‹åŒ–ä¸Šä¸‹æ–‡
            init_start = time.time()
            ctx = self._init_run_context(instance)
            init_elapsed = time.time() - init_start
            self.logger.info(f"[ä¸Šä¸‹æ–‡åˆå§‹åŒ–å®Œæˆ] è€—æ—¶: {init_elapsed:.3f}s")

            # 2. åˆå§‹è¯„ä¼°
            self._perform_initial_evaluation(ctx)

            # 3. ä¼˜åŒ–å¾ªç¯
            loop_start = time.time()
            self._process_optimization_loop(ctx)
            loop_elapsed = time.time() - loop_start
            self.logger.info(f"[ä¼˜åŒ–å¾ªç¯å®Œæˆ] æ€»è€—æ—¶: {loop_elapsed:.2f}s ({loop_elapsed / 60:.1f}åˆ†é’Ÿ)")

            # 4. å®Œæˆå¹¶ç”Ÿæˆç»“æœ
            result = self._finalize_run(ctx)

            run_elapsed = time.time() - run_start_time
            self.logger.info(
                f"\n{'#' * 70}\n"
                f"# [PerfAgent è¿è¡ŒæˆåŠŸå®Œæˆ]\n"
                f"# å®ä¾‹: {instance_id}\n"
                f"# æ€»è€—æ—¶: {run_elapsed:.2f}s ({run_elapsed / 60:.1f}åˆ†é’Ÿ)\n"
                f"# æˆåŠŸ: {result.get('success', False)}\n"
                f"{'#' * 70}"
            )
            return result

        except Exception as e:
            run_elapsed = time.time() - run_start_time
            self.logger.error(
                f"\n{'!' * 70}\n"
                f"! [PerfAgent è¿è¡Œå¤±è´¥]\n"
                f"! å®ä¾‹: {instance_id}\n"
                f"! è¿è¡Œè€—æ—¶: {run_elapsed:.2f}s ({run_elapsed / 60:.1f}åˆ†é’Ÿ)\n"
                f"! é”™è¯¯ç±»å‹: {type(e).__name__}\n"
                f"! é”™è¯¯ä¿¡æ¯: {e}\n"
                f"! å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}\n"
                f"{'!' * 70}"
            )
            # å°è¯•è®°å½•é”™è¯¯è½¨è¿¹
            try:
                # å¦‚æœ ctx å­˜åœ¨ï¼Œå°è¯•ç”¨å®ƒæ¥ç»“æŸè½¨è¿¹
                if "ctx" in locals():
                    ctx.trajectory.finalize(success=False, error_message=str(e), final_submission_code=ctx.best_code)
                    self.logger.info("[è½¨è¿¹å·²ä¿å­˜] é”™è¯¯è½¨è¿¹è®°å½•å®Œæˆ")
            except Exception as traj_error:
                self.logger.warning(f"[è½¨è¿¹ä¿å­˜å¤±è´¥] {type(traj_error).__name__}: {traj_error}")
            raise

    def _init_run_context(self, instance: EffiBenchXInstance) -> RunContext:
        """åˆå§‹åŒ–è¿è¡Œä¸Šä¸‹æ–‡"""
        inst = instance
        instance_id = getattr(inst, "task_name", None) or getattr(inst, "id", "unknown")

        # åˆå§‹åŒ–è½¨è¿¹è®°å½•å™¨
        trajectory = TrajectoryLogger(
            instance_id,
            self.config.logging.trajectory_dir,
            log_dir=self.config.logging.log_dir,
        )

        language = self._normalize_language(self.config.language_cfg.language)
        trajectory.metadata.language = language
        trajectory.metadata.optimization_target = self.config.optimization.target

        # è®°å½• System Prompt
        system_prompt = self._build_system_prompt(
            language=language,
            optimization_target=self.config.optimization.target,
            task_description=inst.description_md,
            task_type=getattr(inst, "type", None),
            starter_code=self._resolve_starter_code(inst, language),
        )
        trajectory.add_history(role="system", content=system_prompt, message_type="system_prompt")

        # æå–åˆå§‹ä»£ç 
        initial_code = self._extract_initial_code(
            inst, language=language, optimization_target=self.config.optimization.target
        )
        if not initial_code:
            raise ValueError("æ— æ³•æå–åˆå§‹ä»£ç ")

        test_cases = self._prepare_test_cases(inst)
        iter_offset = 1 if self._initial_code_source in ("text", "dir") else 0

        # åˆå§‹åŒ–å†å²
        self.optimization_history = []

        return RunContext(
            instance=inst,
            trajectory=trajectory,
            language=language,
            optimization_target=self.config.optimization.target,
            initial_code=initial_code,
            current_code=initial_code,
            best_code=initial_code,
            best_performance=float("inf"),
            best_pass_rate=0.0,
            current_benchmark_results={},
            best_benchmark_results={},
            optimization_history=self.optimization_history,
            iter_offset=iter_offset,
            test_cases=test_cases,
        )

    def _perform_initial_evaluation(self, ctx: RunContext):
        """æ‰§è¡Œåˆå§‹æ€§èƒ½è¯„ä¼°"""
        init_eval_start = time.time()
        self.logger.info(
            f"\n{'=' * 60}\n"
            f"[åˆå§‹è¯„ä¼°å¼€å§‹] æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}\n"
            f"  - å®ä¾‹ID: {ctx.instance.id}\n"
            f"  - è¯­è¨€: {ctx.language}\n"
            f"  - ä¼˜åŒ–ç›®æ ‡: {ctx.optimization_target}\n"
            f"  - åˆå§‹ä»£ç æ¥æº: {self._initial_code_source}\n"
            f"  - åˆå§‹ä»£ç é•¿åº¦: {len(ctx.current_code)} å­—ç¬¦\n"
            f"  - æµ‹è¯•ç”¨ä¾‹æ•°: {len(ctx.test_cases)}\n"
            f"{'=' * 60}"
        )

        step_id = ctx.trajectory.start_step(
            "initial_evaluation", query="Evaluate the initial code performance.", code_snapshot=ctx.current_code
        )

        initial_performance = self._evaluate_performance(ctx.language, ctx.current_code, ctx.test_cases, ctx.instance)

        ctx.current_benchmark_results = initial_performance
        ctx.best_benchmark_results = initial_performance

        initial_evaluation_summary = {
            "performance_analysis": initial_performance.get("performance_analysis", {}),
            "failed_test_details": initial_performance.get("failed_test_details", [])[:3],
        }

        summary_text = self._build_summary_text(
            iteration=1 if ctx.iter_offset else 0,
            code_changed=False,
            diff_text=None,
            benchmark_results=initial_performance,
            current_program=ctx.current_code,
        )

        ctx.trajectory.end_step(
            step_id,
            response=summary_text,
            thought="æ”¶é›†åˆå§‹æ€§èƒ½åŸºçº¿ä»¥æŒ‡å¯¼åç»­ä¼˜åŒ–",
            code_changed=False,
            performance_metrics=initial_evaluation_summary,
            code_snapshot=ctx.current_code,
        )

        ctx.best_pass_rate = self._extract_pass_rate(initial_performance)
        init_metric = initial_performance.get("performance_analysis", {}).get(ctx.optimization_target, float("inf"))

        ctx.initial_performance_value = self._clean_performance_value(init_metric)

        if ctx.initial_performance_value <= ctx.best_performance:
            ctx.best_performance = ctx.initial_performance_value
            ctx.best_code = ctx.current_code

        init_eval_elapsed = time.time() - init_eval_start
        perf_analysis = initial_performance.get("performance_analysis", {})
        self.logger.info(
            f"\n[åˆå§‹è¯„ä¼°å®Œæˆ] æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}, æ€»è€—æ—¶: {init_eval_elapsed:.2f}s\n"
            f"  ğŸ“Š åˆå§‹æ€§èƒ½åŸºçº¿:\n"
            f"      - pass_rate: {ctx.best_pass_rate:.2%}\n"
            f"      - runtime: {perf_analysis.get('runtime', 'N/A')}s\n"
            f"      - memory: {perf_analysis.get('memory', 'N/A')}MB\n"
            f"      - integral: {perf_analysis.get('integral', 'N/A')}MB*s\n"
            f"      - {ctx.optimization_target} (ä¼˜åŒ–ç›®æ ‡): {ctx.initial_performance_value}"
        )

    def _process_optimization_loop(self, ctx: RunContext):
        """æ‰§è¡Œä¼˜åŒ–å¾ªç¯"""
        remaining_iterations = max(0, self.config.max_iterations - ctx.iter_offset)

        self.logger.info(
            f"\n[ä¼˜åŒ–å¾ªç¯å¼€å§‹] è®¡åˆ’è¿­ä»£æ¬¡æ•°: {remaining_iterations}, "
            f"iter_offset: {ctx.iter_offset}, max_iterations: {self.config.max_iterations}"
        )

        for iteration in range(remaining_iterations):
            current_iter_num = iteration + 1 + ctx.iter_offset

            should_stop = self._process_single_iteration(ctx, current_iter_num)
            if should_stop:
                self.logger.info(f"[ä¼˜åŒ–å¾ªç¯æå‰ç»ˆæ­¢] åœ¨ç¬¬ {current_iter_num} æ¬¡è¿­ä»£ååœæ­¢")
                break

        self.logger.info(f"[ä¼˜åŒ–å¾ªç¯ç»“æŸ] å…±æ‰§è¡Œ {len(ctx.optimization_history)} æ¬¡è¿­ä»£")

    def _process_single_iteration(self, ctx: RunContext, iteration_num: int) -> bool:
        """å¤„ç†å•æ¬¡è¿­ä»£ã€‚è¿”å› True è¡¨ç¤ºåº”è¯¥åœæ­¢å¾ªç¯ã€‚"""
        iteration_start_time = time.time()
        self.logger.info(
            f"\n{'=' * 60}\n[è¿­ä»£ {iteration_num} å¼€å§‹] æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}\n{'=' * 60}"
        )

        # 1. ç”Ÿæˆä¼˜åŒ–å»ºè®®
        prompt_start = time.time()
        opt_prompt = self._build_optimization_prompt(
            current_program=ctx.current_code,
            language=ctx.language,
            benchmark_results=ctx.current_benchmark_results,
        )
        prompt_elapsed = time.time() - prompt_start
        self.logger.debug(f"[Promptæ„å»º] è€—æ—¶: {prompt_elapsed:.3f}s, Prompté•¿åº¦: {len(opt_prompt)} å­—ç¬¦")

        step_id = ctx.trajectory.start_step(
            "generate_optimization",
            query=opt_prompt,
            code_snapshot=ctx.current_code,
        )

        # 2. è°ƒç”¨ LLMï¼ˆè¿™é‡Œä¼šæœ‰è¯¦ç»†çš„ LLM æ—¥å¿—ï¼‰
        llm_phase_start = time.time()
        optimization_response = self._call_llm_for_optimization(ctx, opt_prompt)
        llm_phase_elapsed = time.time() - llm_phase_start

        # 3. æå–å’Œåº”ç”¨ä»£ç å˜æ›´
        extract_start = time.time()
        diff_text = None
        optimized_code = None

        if self.config.optimization.code_generation_mode == "direct":
            optimized_code = self._extract_full_code_from_response(optimization_response)
            if not optimized_code:
                extract_elapsed = time.time() - extract_start
                self.logger.warning(
                    f"[ä»£ç æå–å¤±è´¥] è€—æ—¶: {extract_elapsed:.3f}s, æ¨¡å¼: direct, "
                    f"å“åº”é•¿åº¦: {len(optimization_response)} å­—ç¬¦"
                )
                self._handle_failed_code_extraction(
                    ctx, step_id, optimization_response, iteration_num, "æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆçš„å®Œæ•´ä»£ç "
                )
                return False
        else:
            diff_text = self._extract_diff_from_response(optimization_response)
            if not diff_text:
                extract_elapsed = time.time() - extract_start
                self.logger.warning(
                    f"[Diffæå–å¤±è´¥] è€—æ—¶: {extract_elapsed:.3f}s, å“åº”ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„ SEARCH/REPLACE å—"
                )
                self._handle_failed_code_extraction(
                    ctx, step_id, optimization_response, iteration_num, "æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆçš„ diff"
                )
                return False

            # åº”ç”¨ diff
            diff_apply_start = time.time()
            try:
                optimized_code = self.diff_applier.apply_diff(ctx.current_code, diff_text)
                diff_apply_elapsed = time.time() - diff_apply_start
                self.logger.info(f"[Diffåº”ç”¨æˆåŠŸ] è€—æ—¶: {diff_apply_elapsed:.3f}s, diffé•¿åº¦: {len(diff_text)} å­—ç¬¦")
            except Exception as e:
                diff_apply_elapsed = time.time() - diff_apply_start
                self.logger.error(
                    f"[Diffåº”ç”¨å¤±è´¥] è€—æ—¶: {diff_apply_elapsed:.3f}s, é”™è¯¯ç±»å‹: {type(e).__name__}, é”™è¯¯ä¿¡æ¯: {e}"
                )
                self._handle_failed_diff_application(
                    ctx, step_id, optimization_response, diff_text, iteration_num, str(e)
                )
                return False

        extract_elapsed = time.time() - extract_start

        # 4. æ£€æŸ¥ä»£ç æ˜¯å¦å˜åŒ–
        code_changed = optimized_code != ctx.current_code
        code_diff_lines = abs(len(optimized_code.splitlines()) - len(ctx.current_code.splitlines()))
        self.logger.info(
            f"[ä»£ç å˜æ›´æ£€æŸ¥] ä»£ç å·²å˜æ›´: {code_changed}, "
            f"æ–°ä»£ç é•¿åº¦: {len(optimized_code)} å­—ç¬¦, è¡Œæ•°å˜åŒ–: {code_diff_lines:+d}"
        )

        if not code_changed:
            self._handle_no_code_change(ctx, step_id, optimization_response, diff_text, iteration_num)
            ctx.no_improve_count += 1
            iteration_elapsed = time.time() - iteration_start_time
            self.logger.info(
                f"[è¿­ä»£ {iteration_num} ç»“æŸ] ä»£ç æœªå˜æ›´, è·³è¿‡è¯„ä¼°\n"
                f"  - LLMè°ƒç”¨è€—æ—¶: {llm_phase_elapsed:.2f}s ({llm_phase_elapsed / 60:.1f}åˆ†é’Ÿ)\n"
                f"  - ä»£ç æå–è€—æ—¶: {extract_elapsed:.3f}s\n"
                f"  - è¿­ä»£æ€»è€—æ—¶: {iteration_elapsed:.2f}s ({iteration_elapsed / 60:.1f}åˆ†é’Ÿ)\n"
                f"  - è¿ç»­æœªæ”¹è¿›æ¬¡æ•°: {ctx.no_improve_count}"
            )
            if self.config.early_stop_no_improve and ctx.no_improve_count >= self.config.early_stop_no_improve:
                self.logger.info(f"[æå‰åœæ­¢] è¿ç»­æœªæ”¹è¿›è¾¾åˆ°é˜ˆå€¼ {self.config.early_stop_no_improve}")
                return True
            return False

        # 5. è¯„ä¼°æ–°ä»£ç ï¼ˆè¿™é‡Œä¼šæœ‰è¯¦ç»†çš„è¯„æµ‹æ—¥å¿—ï¼‰
        eval_phase_start = time.time()
        try:
            performance_result = self._evaluate_performance(ctx.language, optimized_code, ctx.test_cases, ctx.instance)
            eval_phase_elapsed = time.time() - eval_phase_start

            # æ›´æ–°ä¸Šä¸‹æ–‡çŠ¶æ€
            improved = self._update_run_context_after_eval(
                ctx, optimized_code, performance_result, diff_text, iteration_num
            )

            # è®°å½•æ­¥éª¤
            self._record_iteration_step(
                ctx,
                step_id,
                optimization_response,
                diff_text,
                optimized_code,
                performance_result,
                iteration_num,
                improved,
            )

            if improved:
                ctx.no_improve_count = 0
            else:
                ctx.no_improve_count += 1

            # è¾“å‡ºè¿­ä»£æ€»ç»“
            iteration_elapsed = time.time() - iteration_start_time
            perf_analysis = performance_result.get("performance_analysis", {})
            self.logger.info(
                f"\n[è¿­ä»£ {iteration_num} å®Œæˆ] æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}\n"
                f"  â±ï¸  æ—¶é—´åˆ†è§£:\n"
                f"      - LLMè°ƒç”¨è€—æ—¶: {llm_phase_elapsed:.2f}s ({llm_phase_elapsed / 60:.1f}åˆ†é’Ÿ) "
                f"({llm_phase_elapsed / iteration_elapsed * 100:.1f}%)\n"
                f"      - ä»£ç æå–/åº”ç”¨è€—æ—¶: {extract_elapsed:.3f}s\n"
                f"      - æ€§èƒ½è¯„ä¼°è€—æ—¶: {eval_phase_elapsed:.2f}s ({eval_phase_elapsed / 60:.1f}åˆ†é’Ÿ) "
                f"({eval_phase_elapsed / iteration_elapsed * 100:.1f}%)\n"
                f"      - è¿­ä»£æ€»è€—æ—¶: {iteration_elapsed:.2f}s ({iteration_elapsed / 60:.1f}åˆ†é’Ÿ)\n"
                f"  ğŸ“Š æ€§èƒ½æŒ‡æ ‡:\n"
                f"      - pass_rate: {perf_analysis.get('pass_rate', 0):.2%}\n"
                f"      - runtime: {perf_analysis.get('runtime', 'N/A')}s\n"
                f"      - memory: {perf_analysis.get('memory', 'N/A')}MB\n"
                f"      - integral: {perf_analysis.get('integral', 'N/A')}MB*s\n"
                f"  âœ… ç»“æœ: {'æ€§èƒ½æ”¹è¿›ï¼Œå·²é‡‡çº³' if improved else 'æœªæ”¹è¿›'}\n"
                f"  ğŸ“ˆ è¿ç»­æœªæ”¹è¿›æ¬¡æ•°: {ctx.no_improve_count}"
            )

            if self.config.early_stop_no_improve and ctx.no_improve_count >= self.config.early_stop_no_improve:
                self.logger.info(f"[æå‰åœæ­¢] è¿ç»­æœªæ”¹è¿›è¾¾åˆ°é˜ˆå€¼ {self.config.early_stop_no_improve}")
                return True

        except Exception as e:
            eval_phase_elapsed = time.time() - eval_phase_start
            iteration_elapsed = time.time() - iteration_start_time
            self.logger.error(
                f"[è¿­ä»£ {iteration_num} å¼‚å¸¸] è¯„ä¼°é˜¶æ®µå‡ºé”™\n"
                f"  - é”™è¯¯ç±»å‹: {type(e).__name__}\n"
                f"  - é”™è¯¯ä¿¡æ¯: {e}\n"
                f"  - LLMè°ƒç”¨è€—æ—¶: {llm_phase_elapsed:.2f}s\n"
                f"  - è¯„ä¼°è€—æ—¶(è‡³å¼‚å¸¸): {eval_phase_elapsed:.2f}s\n"
                f"  - è¿­ä»£æ€»è€—æ—¶: {iteration_elapsed:.2f}s\n"
                f"  - å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}"
            )
            self._handle_evaluation_error(ctx, step_id, optimization_response, diff_text, iteration_num, str(e))

        return False

    def _call_llm_for_optimization(self, ctx: RunContext, opt_prompt: str) -> str:
        """è°ƒç”¨ LLM è·å–ä¼˜åŒ–å»ºè®®"""
        system_prompt = self._build_system_prompt(
            language=ctx.language,
            optimization_target=self.config.optimization.target,
            task_description=ctx.instance.description_md,
            task_type=getattr(ctx.instance, "type", None),
            starter_code=self._resolve_starter_code(ctx.instance, ctx.language),
        )
        messages = self._build_messages(system_prompt, ctx.trajectory.history, opt_prompt)

        if self.llm_client:
            llm_start_time = time.time()
            self.logger.info(
                f"[LLMè°ƒç”¨å¼€å§‹] æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}, æ¨¡å‹: {self.config.model.name}"
            )
            try:
                response = self.llm_client.call_llm(
                    messages,
                    temperature=self.config.model.temperature,
                    max_tokens=self.config.model.max_output_tokens,
                    usage_context="perfagent.optimize",
                )
                llm_elapsed = time.time() - llm_start_time
                self.logger.info(
                    f"[LLMè°ƒç”¨å®Œæˆ] è€—æ—¶: {llm_elapsed:.2f}s ({llm_elapsed / 60:.1f}åˆ†é’Ÿ), "
                    f"å“åº”é•¿åº¦: {len(response)} å­—ç¬¦"
                )
                return response
            except Exception as e:
                llm_elapsed = time.time() - llm_start_time
                self.logger.error(
                    f"[LLMè°ƒç”¨å¤±è´¥] è€—æ—¶: {llm_elapsed:.2f}s, é”™è¯¯ç±»å‹: {type(e).__name__}, "
                    f"é”™è¯¯ä¿¡æ¯: {e}\n{traceback.format_exc()}"
                )
                raise
        self.logger.warning("[LLMæœªé…ç½®] LLM å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æœ¬æ¬¡ä¼˜åŒ–")
        return "LLM æœªé…ç½®æˆ–ä¸å¯ç”¨ï¼Œè·³è¿‡æœ¬æ¬¡ä¼˜åŒ–å»ºè®®ã€‚è¯·æ£€æŸ¥ API é…ç½®ã€‚"

    def _handle_failed_code_extraction(
        self, ctx: RunContext, step_id: str, response: str, iteration: int, error_msg: str
    ):
        summary = self._build_summary_text(
            iteration=iteration,
            code_changed=False,
            diff_text=None,
            benchmark_results=None,
            current_program=ctx.current_code,
            error_message=error_msg,
        )
        ctx.trajectory.end_step(
            step_id,
            response=response,
            thought="æœªèƒ½æå–æœ‰æ•ˆçš„ä»£ç /diff",
            code_changed=False,
            diff=None,
            error=error_msg,
            code_snapshot=ctx.current_code,
            summary=summary,
        )

    def _handle_failed_diff_application(
        self, ctx: RunContext, step_id: str, response: str, diff_text: str, iteration: int, error_msg: str
    ):
        summary = self._build_summary_text(
            iteration=iteration,
            code_changed=False,
            diff_text=diff_text,
            benchmark_results=None,
            current_program=ctx.current_code,
            error_message=f"åº”ç”¨ diff å¤±è´¥: {error_msg}",
        )
        ctx.trajectory.end_step(
            step_id,
            response=response,
            thought="åº”ç”¨ diff é˜¶æ®µå‘ç”Ÿå¼‚å¸¸",
            code_changed=None,
            diff=diff_text,
            performance_metrics=None,
            error=f"åº”ç”¨ diff å¤±è´¥: {error_msg}",
            code_snapshot=ctx.current_code,
            summary=summary,
        )

    def _handle_no_code_change(self, ctx: RunContext, step_id: str, response: str, diff_text: str, iteration: int):
        summary = self._build_summary_text(
            iteration=iteration,
            code_changed=False,
            diff_text=diff_text,
            benchmark_results=ctx.current_benchmark_results,
            current_program=ctx.current_code,
        )
        ctx.trajectory.end_step(
            step_id,
            response=response,
            thought="diff åº”ç”¨åä»£ç æœªå˜åŒ–ï¼Œè·³è¿‡",
            code_changed=False,
            diff=diff_text,
            code_snapshot=ctx.current_code,
            summary=summary,
        )
        self.logger.warning("ä»£ç æœªå‘ç”Ÿå˜åŒ–ï¼Œè·³è¿‡æ­¤æ¬¡è¿­ä»£")

    def _update_run_context_after_eval(
        self, ctx: RunContext, optimized_code: str, performance_result: dict, diff_text: str | None, iteration: int
    ) -> bool:
        """æ›´æ–°ä¸Šä¸‹æ–‡å¹¶åˆ¤æ–­æ˜¯å¦æ”¹è¿›"""
        current_performance = performance_result.get("performance_analysis", {}).get(
            ctx.optimization_target, float("inf")
        )
        current_pass_rate = self._extract_pass_rate(performance_result)

        improved = False
        if current_pass_rate == 1.0 and current_performance < ctx.best_performance:
            improved = True

        # å¦‚æœæœ€å¤§è¿­ä»£æ¬¡æ•°ä¸º 1ï¼Œå¼ºåˆ¶è§†ä¸ºæ”¹è¿›ï¼ˆå³æ€»æ˜¯ä¿å­˜ç”Ÿæˆä»£ç ï¼‰
        # é™¤éä»£ç å®Œå…¨å´©æºƒï¼ˆè¿™é‡Œå¯ä»¥æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼Œå½“å‰é€»è¾‘æ˜¯åªè¦è¿è¡Œäº†å°±ä¿å­˜ï¼‰
        if self.config.max_iterations == 1 and not improved:
            improved = True
            self.logger.info("å•æ¬¡è¿­ä»£æ¨¡å¼ï¼šå¼ºåˆ¶é‡‡çº³ç”Ÿæˆä»£ç ä½œä¸ºæœ€ä½³ç»“æœ")

        # è®°å½•å†å²
        ctx.optimization_history.append(
            {
                "iteration": iteration,
                "diff": diff_text,
                "performance_before": ctx.best_performance,
                "performance_after": current_performance,
                "improvement": ctx.best_performance - current_performance,
                "success": improved,
            }
        )

        if improved:
            ctx.best_pass_rate = current_pass_rate
            ctx.best_performance = current_performance
            ctx.best_code = optimized_code
            ctx.best_benchmark_results = performance_result
            self.logger.info(
                f"é‡‡ç”¨æ›´ä¼˜ä»£ç : pass_rate {ctx.best_pass_rate:.2f}, {ctx.optimization_target} {ctx.best_performance:.4f}"
            )
        else:
            self.logger.info(f"æœªæ”¹è¿›: pass_rate {current_pass_rate:.2f} vs {ctx.best_pass_rate:.2f}")

        # å†³å®šæ˜¯å¦é‡‡ç”¨ä»£ç 
        if self.config.optimization.adopt_only_if_improved:
            if improved:
                ctx.current_code = optimized_code
            else:
                ctx.current_code = ctx.best_code
        else:
            ctx.current_code = optimized_code

        ctx.current_benchmark_results = performance_result
        return improved

    def _record_iteration_step(
        self,
        ctx: RunContext,
        step_id: str,
        response: str,
        diff_text: str | None,
        optimized_code: str,
        performance_result: dict,
        iteration: int,
        improved: bool,
    ):
        adopted = improved if self.config.optimization.adopt_only_if_improved else True

        evaluation_summary = {
            "performance_analysis": performance_result.get("performance_analysis", {}),
            "failed_test_details": performance_result.get("failed_test_details", [])[:3],
            "pass_rates": performance_result.get("pass_rates", []),
            "pass_rate_consistent": performance_result.get("pass_rate_consistent", False),
        }

        summary_text = self._build_summary_text(
            iteration=iteration,
            code_changed=adopted,
            diff_text=diff_text,
            benchmark_results=performance_result,
            current_program=ctx.current_code,
        )

        ctx.trajectory.end_step(
            step_id,
            response=response,
            thought=("åº”ç”¨ diff å¹¶å®Œæˆæ€§èƒ½è¯„ä¼°" if adopted else "è¯„ä¼°æœªæ”¹è¿›ï¼Œæœªé‡‡ç”¨ä¼˜åŒ–"),
            code_changed=adopted,
            diff=diff_text,
            performance_metrics=evaluation_summary,
            code_snapshot=ctx.current_code,
            summary=summary_text,
        )

    def _handle_evaluation_error(
        self, ctx: RunContext, step_id: str, response: str, diff_text: str | None, iteration: int, error_msg: str
    ):
        summary = self._build_summary_text(
            iteration=iteration,
            code_changed=True,
            diff_text=diff_text,
            benchmark_results=None,
            current_program=ctx.current_code,
            error_message=f"æ€§èƒ½è¯„ä¼°å¤±è´¥: {error_msg}",
        )
        ctx.trajectory.end_step(
            step_id,
            response=response,
            thought="æ€§èƒ½è¯„ä¼°é˜¶æ®µå‘ç”Ÿå¼‚å¸¸",
            code_changed=True,
            diff=diff_text,
            performance_metrics=None,
            error=f"æ€§èƒ½è¯„ä¼°å¤±è´¥: {error_msg}",
            code_snapshot=ctx.current_code,
            summary=summary,
        )

    def _finalize_run(self, ctx: RunContext) -> dict[str, Any]:
        """å®Œæˆè¿è¡Œå¹¶ç”Ÿæˆæœ€ç»ˆç»“æœ"""
        finalize_start = time.time()
        self.logger.info(f"\n[ç»“æœæ±‡æ€»å¼€å§‹] æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")

        initial_trimmed = ctx.initial_performance_value
        best_perf = self._clean_performance_value(ctx.best_performance)

        executed_iterations = len(ctx.optimization_history)
        # åˆå§‹ä»£ç  + è¿­ä»£æ¬¡æ•°
        total_iterations = (1 if self._initial_code_source in ("text", "dir") else 0) + executed_iterations

        optimized_code_final = ctx.best_code

        final_result = {
            "instance_id": ctx.instance.id,
            "initial_code": ctx.initial_code,
            "optimized_code": optimized_code_final,
            "initial_performance": initial_trimmed,
            "final_performance": best_perf,
            "total_iterations": total_iterations,
            "optimization_history": ctx.optimization_history,
            "success": bool(best_perf < initial_trimmed),
        }

        unit = (
            "s" if ctx.optimization_target == "runtime" else ("MB" if ctx.optimization_target == "memory" else "MB*s")
        )
        final_result["language"] = ctx.language
        final_result["optimization_target"] = ctx.optimization_target
        final_result["performance_unit"] = unit

        try:
            result_for_output = ctx.best_benchmark_results
            metrics_dict, artifacts_dict = self._build_metrics_and_artifacts(result_for_output)
            metrics_md = self._format_metrics_md(metrics_dict)
            artifacts_md = self._format_artifacts_md(artifacts_dict)

            final_artifacts = "Current Metrics:\n" + metrics_md + "\n\nCurrent Artifacts:\n" + artifacts_md
            final_result["final_artifacts"] = final_artifacts
        except Exception as e:
            self.logger.warning(f"[æ„å»ºæœ€ç»ˆæŒ‡æ ‡å¤±è´¥] {type(e).__name__}: {e}")
            final_result["final_artifacts"] = None

        # æ±‡æ€»æœ€ç»ˆä¸‰é¡¹æŒ‡æ ‡
        try:
            perf_metrics = result_for_output.get("performance_analysis", {})
            final_result["final_metrics"] = {
                "runtime": perf_metrics.get("runtime", "Infinity"),
                "memory": perf_metrics.get("memory", "Infinity"),
                "integral": perf_metrics.get("integral", "Infinity"),
            }
        except Exception as e:
            self.logger.warning(f"[è·å–æ€§èƒ½æŒ‡æ ‡å¤±è´¥] {type(e).__name__}: {e}")
            final_result["final_metrics"] = {
                "runtime": "Infinity",
                "memory": "Infinity",
                "integral": "Infinity",
            }

        # è®°å½•æœ€ç»ˆè½¨è¿¹
        selected_value = final_result.get("final_metrics", {}).get(ctx.optimization_target)
        selected_value = self._clean_performance_value(selected_value)

        trajectory_file = ctx.trajectory.finalize(
            success=final_result["success"],
            final_performance={
                "target": ctx.optimization_target,
                "unit": unit,
                "value": selected_value if selected_value != float("inf") else best_perf,
                "runtime": final_result.get("final_metrics", {}).get("runtime"),
                "memory": final_result.get("final_metrics", {}).get("memory"),
                "integral": final_result.get("final_metrics", {}).get("integral"),
            },
            final_submission_code=optimized_code_final,
        )

        final_result["trajectory_file"] = trajectory_file

        # è®¡ç®—æ”¹è¿›å¹…åº¦
        improvement_pct = 0.0
        if initial_trimmed != float("inf") and initial_trimmed > 0:
            improvement_pct = (initial_trimmed - best_perf) / initial_trimmed * 100

        # ç»Ÿè®¡ä¼˜åŒ–å†å²
        successful_iterations = sum(1 for h in ctx.optimization_history if h.get("success", False))

        finalize_elapsed = time.time() - finalize_start
        self.logger.info(
            f"\n[ä¼˜åŒ–ç»“æœæ€»ç»“]\n"
            f"  ğŸ“‹ åŸºæœ¬ä¿¡æ¯:\n"
            f"      - å®ä¾‹ID: {ctx.instance.id}\n"
            f"      - è¯­è¨€: {ctx.language}\n"
            f"      - ä¼˜åŒ–ç›®æ ‡: {ctx.optimization_target}\n"
            f"      - æ‰§è¡Œè¿­ä»£æ•°: {executed_iterations}\n"
            f"      - æˆåŠŸæ”¹è¿›è¿­ä»£æ•°: {successful_iterations}\n"
            f"\n"
            f"  ğŸ“ˆ æ€§èƒ½å˜åŒ–:\n"
            f"      - åˆå§‹ {ctx.optimization_target}: {initial_trimmed} {unit}\n"
            f"      - æœ€ç»ˆ {ctx.optimization_target}: {best_perf} {unit}\n"
            f"      - æ”¹è¿›å¹…åº¦: {improvement_pct:.2f}%\n"
            f"      - ä¼˜åŒ–æˆåŠŸ: {'âœ… æ˜¯' if final_result['success'] else 'âŒ å¦'}\n"
            f"\n"
            f"  ğŸ“Š æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡:\n"
            f"      - runtime: {final_result['final_metrics']['runtime']}s\n"
            f"      - memory: {final_result['final_metrics']['memory']}MB\n"
            f"      - integral: {final_result['final_metrics']['integral']}MB*s\n"
            f"      - pass_rate: {ctx.best_pass_rate:.2%}\n"
            f"\n"
            f"  ğŸ“ è½¨è¿¹æ–‡ä»¶: {trajectory_file}\n"
            f"  â±ï¸  ç»“æœæ±‡æ€»è€—æ—¶: {finalize_elapsed:.3f}s"
        )

        return final_result

    def _build_optimization_prompt(
        self,
        current_program: str,
        language: str,
        benchmark_results: dict[str, Any],
    ) -> str:
        """æ„å»ºä¼˜åŒ–æç¤ºè¯ï¼Œå¡«å……å½“å‰ç¨‹åºã€è¯„ä¼°æŒ‡æ ‡ä¸æ„ä»¶(section)ã€‚"""
        if self.config.optimization.code_generation_mode == "direct":
            return self.config.prompts.optimization_template

        # diff-based prompt construction
        # æ„é€  metrics ä¸ artifacts
        metrics_dict, artifacts_dict = self._build_metrics_and_artifacts(benchmark_results)
        # ä»¥ Markdown æ ¼å¼åŒ–ï¼Œä¾¿äºæ¨¡å‹é˜…è¯»
        current_metrics_str = self._format_metrics_md(metrics_dict)
        current_artifacts_str = self._format_artifacts_md(artifacts_dict)
        current_program_md = f"```\n{current_program}\n```"

        try:
            return self.config.prompts.optimization_template.format(
                current_program=current_program_md,
                current_metrics=current_metrics_str,
                current_artifacts_section=current_artifacts_str,
                language=language,
            )
        except Exception:
            # è‹¥æ¨¡æ¿å ä½ç¬¦ä¸åŒ¹é…ï¼Œå›é€€ä¸ºä¸€ä¸ªé€šç”¨æç¤º
            return (
                "# Task\n"
                "è¯·åˆ†æä»¥ä¸‹ç¨‹åºä¿¡æ¯ï¼Œå¹¶æ ¹æ®ç³»ç»Ÿæç¤ºç”Ÿæˆ `## Thinking` ä¸ `## Diffs`ï¼š\n\n"
                "## Current Program\n" + current_program_md + "\n\n"
                "## Current Metrics\n" + current_metrics_str + "\n\n"
                "## Current Artifacts\n" + current_artifacts_str
            )

    def _build_system_prompt(
        self,
        language: str,
        optimization_target: str,
        task_description: str,
        task_type: str | None = None,
        starter_code: str | None = None,
    ) -> str:
        tmpl = self.config.prompts.system_template
        additional = self.config.prompts.additional_requirements or ""
        local_memory = getattr(self.config.prompts, "local_memory", None) or ""
        global_memory = getattr(self.config.prompts, "global_memory", None) or ""
        allowed_imports_scope = EFFIBENCH_REGISTRY.get(language, {}).get("imports", "")
        is_functional = (task_type or "").lower() == "functional"
        if tmpl:
            try:
                base = tmpl.format(
                    language=language,
                    optimization_target=optimization_target,
                    task_description=task_description,
                    additional_requirements=additional,
                    local_memory=local_memory,
                    global_memory=global_memory,
                    allowed_imports_scope=allowed_imports_scope,
                )
            except Exception:
                base = tmpl
        else:
            base = (
                f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç æ€§èƒ½ä¼˜åŒ–ä¸“å®¶ã€‚ç›®æ ‡æ˜¯æå‡ {optimization_target}ã€‚\n"
                f"å½“å‰è¯­è¨€ï¼š{language}ã€‚ä»»åŠ¡æè¿°ï¼š{task_description}\n\n"
                f"é™„åŠ è¦æ±‚ï¼š{additional}\n\n"
                f"æœ¬åœ°è®°å¿†ï¼š{local_memory}\n\n"
                f"å…¨å±€è®°å¿†ï¼š{global_memory}\n\n"
                f"å…è®¸ä½¿ç”¨çš„æ ‡å‡†å¯¼å…¥èŒƒå›´å¦‚ä¸‹ï¼š\n"
                f"{allowed_imports_scope}"
            )
        if is_functional and starter_code:
            starter_section = (
                "\n\n## Starter Code\n"
                "Use the following starter code as the exact framework for your solution.\n\n"
                f"```{language}\n"
                f"{starter_code}\n"
                "```\n\n"
                "- Implement the function with the exact signature (name, parameters, etc.) "
                "specified in the starter code.\n"
            )
            return base + starter_section
        return base

    def _build_metrics_and_artifacts(
        self, benchmark_results: dict[str, Any], include_other_metrics: bool | None = None
    ) -> tuple[dict[str, Any], dict[str, Any]]:
        """æ ¹æ®åŸºå‡†è¯„ä¼°ç»“æœæ„é€  current_metrics ä¸ current_artifacts_sectionã€‚"""
        performance_metrics = benchmark_results.get("performance_analysis", {})
        failed_test_details = benchmark_results.get("failed_test_details", []) or []

        # å¤±è´¥æƒ…å†µï¼šæ±‡æ€»å¤±è´¥ä¿¡æ¯å¹¶è¿”å›é”™è¯¯æŒ‡æ ‡
        target = self.config.optimization.target

        # Determine which metrics to include
        if include_other_metrics is None:
            include_other_metrics = self.config.optimization.include_other_metrics_in_summary

        keys_to_include = {"runtime", "memory", "integral"}
        if not include_other_metrics:
            keys_to_include = {target}

        # å¤±è´¥æƒ…å†µï¼šæ±‡æ€»å¤±è´¥ä¿¡æ¯å¹¶è¿”å›é”™è¯¯æŒ‡æ ‡
        passed = performance_metrics.get("passed", False)
        if not passed:
            num_failed = len(failed_test_details)
            num_total = len(benchmark_results.get("first_run_details", []))
            pass_rate = (num_total - num_failed) / num_total if num_total > 0 else 0

            representative_failures: dict[str, Any] = {}
            for failure in failed_test_details:
                status = failure.get("status", "unknown")
                if status not in representative_failures:
                    representative_failures[status] = failure

            failure_details_summary: list[str] = []
            for status, failure in representative_failures.items():
                text = failure.get("text", "No additional error text.")
                if isinstance(text, str) and len(text) > 300:
                    text = text[-300:] + "..."
                failure_details_summary.append(f"- Status: {status}, Details (last 300 chars of Output): {text}")

            failures_text = "\n".join(failure_details_summary)
            all_statuses = ", ".join(representative_failures.keys())

            error_artifacts = {
                "error_type": f"SolutionFailedTests (statuses: {all_statuses})",
                "error_message": (f"Solution passed {pass_rate:.2%} of test cases. Failure details:\n{failures_text}"),
                "suggestion": (
                    "Review the solution to ensure it correctly handles all test cases, including edge cases."
                ),
            }

            metrics = {
                "pass_rate": pass_rate,
                "target": target,
                "error": (
                    f"Solution failed {len(failed_test_details)} test case(s) with statuses: {all_statuses}. See artifacts for details."
                ),
            }
            for k in keys_to_include:
                metrics[k] = "Infinity"

            return metrics, error_artifacts

        # æˆåŠŸæƒ…å†µï¼šè®¡ç®—æ—¶é—´åˆ†æ•°ä¸ç»¼åˆåˆ†æ•°
        pass_rate = 1.0

        metrics = {
            "pass_rate": pass_rate,
            "target": target,
        }
        for k in keys_to_include:
            metrics[k] = performance_metrics.get(k, "Infinity")

        artifacts = {"details": "All test cases passed."}
        return metrics, artifacts

    def _format_metrics_md(self, metrics: dict[str, Any]) -> str:
        """å°†æ€§èƒ½æŒ‡æ ‡æ ¼å¼åŒ–ä¸º Markdown æ–‡æœ¬ã€‚"""
        lines: list[str] = []

        pr = metrics.get("pass_rate")
        if pr is not None:
            try:
                pr_pct = f"{float(pr) * 100:.2f}%"
            except Exception:
                pr_pct = str(pr)
            lines.append(f"- Pass rate: {pr_pct}")

        def _fmt(val: Any, unit: str) -> str:
            if isinstance(val, (int, float)):
                if val == float("inf"):
                    return "Infinity"
                if val == float("-inf"):
                    return "-Infinity"
                return f"{float(val):.6f} {unit}"
            s = str(val).strip().lower()
            if s in ("inf", "+inf", "infinity", "+infinity"):
                return "Infinity"
            if s in ("-inf", "-infinity"):
                return "-Infinity"
            if s == "nan":
                return "NaN"
            try:
                return f"{float(val):.6f} {unit}"
            except Exception:
                return f"{val} {unit}"

        if "runtime" in metrics:
            lines.append(f"- Runtime: {_fmt(metrics.get('runtime'), 's')}")
        if "memory" in metrics:
            lines.append(f"- Memory: {_fmt(metrics.get('memory'), 'MB')}")
        if "integral" in metrics:
            lines.append(f"- Integral: {_fmt(metrics.get('integral'), 'MB*s')}")

        tgt = metrics.get("target")
        if tgt is not None:
            lines.append(f"- Target: {tgt}")

        err = metrics.get("error")
        if err:
            lines.append(f"- Error: {err}")

        return "\n".join(lines) if lines else "- No metrics available."

    def _format_artifacts_md(self, artifacts: dict[str, Any]) -> str:
        """å°†æ„ä»¶ä¿¡æ¯æ ¼å¼åŒ–ä¸º Markdown æ–‡æœ¬ã€‚"""
        if not artifacts:
            return "- No artifacts available."
        lines: list[str] = []
        for k, v in artifacts.items():
            if isinstance(v, str) and "\n" in v:
                indented = "\n  ".join(v.split("\n"))
                lines.append(f"- {k}: {indented}")
            else:
                lines.append(f"- {k}: {v}")
        return "\n".join(lines)

    def _build_summary_text(
        self,
        iteration: int,
        code_changed: bool,
        diff_text: str | None,
        benchmark_results: dict[str, Any] | None,
        current_program: str | None = None,
        error_message: str | None = None,
    ) -> str:
        """æ„å»ºä¸€æ­¥è¿­ä»£çš„ Markdown æ‘˜è¦æ–‡æœ¬ï¼ŒåŒ…å«ç¨‹åºæ›´æ–°ã€å½“å‰ç¨‹åºã€æŒ‡æ ‡ä¸æ„ä»¶ã€‚

        - metrics/artifacts ç”± `_build_metrics_and_artifacts` ç”Ÿæˆå¹¶é€šè¿‡ `_format_*_md` æ ¼å¼åŒ–ã€‚
        - æ— è¯„ä¼°æˆ–å¤±è´¥æ—¶ï¼Œè¾“å‡ºé”™è¯¯ä¿¡æ¯å’Œå ä½æ„ä»¶ã€‚
        """
        # æ„é€ æŒ‡æ ‡ä¸æ„ä»¶
        if benchmark_results:
            metrics_dict, artifacts_dict = self._build_metrics_and_artifacts(benchmark_results)
        else:
            metrics_dict = {}
            artifacts_dict = {}
            if error_message:
                metrics_dict["error"] = error_message
                if not artifacts_dict:
                    artifacts_dict["details"] = "No evaluation due to error."

        metrics_md = self._format_metrics_md(metrics_dict)
        artifacts_md = self._format_artifacts_md(artifacts_dict)
        diff_size = len(diff_text) if diff_text else 0

        prog_text = current_program or ""

        return (
            "## Program Update\n"
            f"- Iteration: {iteration}\n"
            "## Current Program\n" + prog_text + "\n\n"
            "## Current Metrics\n" + metrics_md + "\n\n"
            "## Current Artifacts\n" + artifacts_md
        )

    def _extract_full_code_from_response(self, response: str) -> str:
        """ä»æ¨¡å‹å“åº”ä¸­æå–å®Œæ•´ä»£ç ï¼ˆMarkdown ä»£ç å—ï¼‰ã€‚"""
        if not response:
            return ""
        # åŒ¹é… ```language ... ```
        # å°è¯•åŒ¹é… python, cpp, java, etc. æˆ–è€…ä¸æŒ‡å®š
        pattern = r"```(?:\w+)?\n(.*?)```"
        matches = re.findall(pattern, response, re.DOTALL)
        if matches:
            # è¿”å›æœ€åä¸€ä¸ªåŒ¹é…çš„ä»£ç å—ï¼Œé€šå¸¸æ˜¯æœ€ç»ˆä»£ç 
            return matches[-1].strip()
        return ""

    def _extract_diff_from_response(self, response: str) -> str:
        """ä»æ¨¡å‹å“åº”ä¸­æå– diff
        ä»…æ”¯æŒ SEARCH/REPLACE åŒºå—æ ¼å¼ã€‚
        """
        if not response:
            return ""
        if "<<<<<<< SEARCH" in response and ">>>>>>> REPLACE" in response:
            try:
                start_idx = response.find("<<<<<<< SEARCH")
                end_idx = response.rfind(">>>>>>> REPLACE")
                if start_idx != -1 and end_idx != -1 and end_idx >= start_idx:
                    return response[start_idx : end_idx + len(">>>>>>> REPLACE")].strip()
            except Exception:
                return ""
        return ""

    def _build_messages(
        self, system_prompt: str, history: list[dict[str, Any]], user_prompt: str, limit: int = 200
    ) -> list[dict[str, str]]:
        use_all = bool(getattr(self.config.prompts, "include_all_history", False))
        if use_all:
            msgs: list[dict[str, str]] = []
            tail = history[-limit:] if len(history) > limit else history
            for h in tail:
                role = h.get("role")
                content = h.get("content", "")
                if role in ("system", "user", "assistant") and content:
                    msgs.append({"role": role, "content": content})
            return msgs
        return [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]

    def run_with_request(self, request: "PerfAgentRequest") -> "PerfAgentResult":
        """æ ‡å‡†åŒ– API å…¥å£ï¼Œæ¥æ”¶ Request è¿”å› Result

        è¿™æ˜¯ SE_Perf ä¸ PerfAgent ä¹‹é—´çš„æ ‡å‡†åŒ–æ¥å£ã€‚
        æ¥æ”¶ PerfAgentRequestï¼Œåº”ç”¨è¦†ç›–é…ç½®ï¼Œæ‰§è¡Œä¼˜åŒ–ï¼Œè¿”å› PerfAgentResultã€‚

        Args:
            request: PerfAgentRequest å¯¹è±¡ï¼ŒåŒ…å«å®ä¾‹ã€é…ç½®å’Œè¦†ç›–å‚æ•°

        Returns:
            PerfAgentResult å¯¹è±¡ï¼ŒåŒ…å«ä¼˜åŒ–ç»“æœ
        """
        from .protocols import PerfAgentRequest, PerfAgentResult

        # åº”ç”¨è¯·æ±‚ä¸­çš„è¦†ç›–å‚æ•°åˆ°é…ç½®
        request.apply_overrides()

        # å¦‚æœè¯·æ±‚æŒ‡å®šäº†è¾“å‡ºç›®å½•ï¼Œæ›´æ–°é…ç½®
        if request.output_dir:
            self.config.logging.trajectory_dir = request.output_dir
            self.config.logging.log_dir = request.output_dir

        try:
            # è°ƒç”¨ç°æœ‰çš„ run æ–¹æ³•
            raw_result = self.run(request.instance)

            # è½¬æ¢ä¸ºæ ‡å‡†åŒ– Result
            return PerfAgentResult.from_dict(raw_result)

        except Exception as e:
            self.logger.error(f"[run_with_request å¼‚å¸¸] {type(e).__name__}: {e}", exc_info=True)
            instance_id = getattr(request.instance, "task_name", None) or getattr(
                request.instance, "id", "unknown"
            )
            return PerfAgentResult.from_error(instance_id=instance_id, error=str(e))
